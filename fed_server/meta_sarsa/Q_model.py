import pandas as pd
from tensorflow import keras
from tensorflow.keras import optimizers

from sklearn.model_selection import train_test_split

from utils_QModel import QModel
# ä¾èµ–ï¼šNUM_SWITCH, DATA_DIR, SEQ_LEN, NUM_FEATURES ç­‰
from utils_fisher import *                 # å¦‚æœæ— éœ€æœ¬æ–‡ä»¶çš„å‡½æ•°ï¼Œå¯ä»¥ç§»é™¤è¿™è¡Œ
from plant_analysis import *
# ============== é…ç½® ==============
ENCODER_LATENT_DIM = 16
Q_HIDDEN = [64, 64]

# æ³¨æ„ï¼šNUM_ACTIONS å¿…é¡»ç”± NUM_SWITCH æ¨å¯¼
NUM_ACTIONS = 2 ** NUM_SWITCH

LR_Q = 1e-3
SARSA_EPISODES = 200
BATCH_MAX = 32
GAMMA = 0.95
EPS_START = 0.3
EPS_END = 0.1
EPS_DECAY = 0.995
ACTION_COST = 0.05

# ============== æ··åˆç²¾åº¦ï¼ˆæ­¤å¤„ä¿æŒ float32 æ›´ç¨³ï¼‰ ==============
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('float32')

# æ•°æ®æ–‡ä»¶åˆ—è¡¨
files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(".csv")]

def normalize_row(row):
    row = row.astype(np.float32)
    row[0] /= 40.0     # temp
    row[1] /= 100.0    # humid
    row[2] /= 1000.0   # light
    return row

def make_windows(X, seq_len, step_size=1):
    """
    X: (T, F) -> windows: (N, seq_len, F), idx: list of start indices
    N = floor((T - seq_len)/step_size) + 1
    """
    T = X.shape[0]
    if T < seq_len:
        return np.empty((0, seq_len, X.shape[1]), dtype=np.float32), []
    starts = list(range(0, T - seq_len + 1, step_size))
    windows = np.stack([X[s:s+seq_len] for s in starts], axis=0).astype(np.float32)
    return windows, starts


import numpy as np
import json
import os
from collections import defaultdict
#print(f"âœ… QModel åˆå§‹åŒ–å®Œæˆ: å‹•ä½œæ•¸={action_size}, åƒæ•¸={params}")

import time
from datetime import datetime
import matplotlib.pyplot as plt

class QModel_aug(QModel):
    def __init__(self, state_size=3, action_size=16, learning_rate=0.1, gamma=0.9,
                 epsilon=0.3, config_file="best_params.json"):
        self.latent_dim=ENCODER_LATENT_DIM
        self.q_net = self.build_q_network(self.latent_dim, action_size)
        self.optimizer = optimizers.Adam(LR_Q)
        self.eps = EPS_START
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate  # å±æ€§åæ˜¯ learning_rate
        self.gamma = gamma  # å±æ€§åæ˜¯ gamma
        self.epsilon = epsilon
        self.q_table = np.zeros((state_size, action_size))  # æˆ–è€…å…¶ä»–çš„Qå‡½æ•°è¡¨ç¤º

        """
        åˆå§‹åŒ– Q-learning æ¨¡å‹
        """
        self.config_file = config_file


        # åŠ è¼‰æˆ–è¨­ç½®åƒæ•¸
        self.best_params = self._load_best_params()
        params = self._get_training_params()

        self.learning_rate = params.get('learning_rate', learning_rate)
        self.gamma = params.get('gamma', gamma)
        self.epsilon = params.get('initial_epsilon', epsilon)
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995

        # åˆå§‹åŒ– Q-tableï¼šæ¯å€‹ç‹€æ…‹å°æ‡‰ä¸€å€‹å¤§å°ç‚º action_size çš„æ•¸çµ„
        self.q_table = defaultdict(lambda: np.zeros(action_size))

        print(f"âœ… QModel åˆå§‹åŒ–å®Œæˆ: å‹•ä½œæ•¸={action_size}, åƒæ•¸={params}")




        self.exploration_stats = {
            'total_choices': 0,
            'exploration_choices': 0,
            'exploitation_choices': 0,
            'action_distribution': np.zeros(action_size),
            'last_choice_type': None
        }


        # è¨“ç·´éç¨‹ç›£æ§
        self.training_history = {
            'episodes': [],
            'rewards': [],
            'avg_rewards': [],
            'epsilons': [],
            'q_table_sizes': [],
            'exploration_rates': [],
            'losses': [],
            'timestamps': [],
            'action_distributions': []
        }

        self.current_episode = 0
        self.episode_rewards = []
        self.episode_start_time = time.time()

        print(f"âœ… QModel åˆå§‹åŒ–å®Œæˆ: å‹•ä½œæ•¸={action_size}, åƒæ•¸={params}")
        super().__init__()
        self.meta_model = keras.models.load_model("meta_model.h5")
        self.hvac_dense_layer = self.meta_model.get_layer("hvac_dense")  # lstm_encoder classifier
        self.encoder = keras.models.Model(inputs=self.meta_model.input, outputs=self.hvac_dense_layer.output)
    def start_episode(self):
        """é–‹å§‹æ–°çš„è¨“ç·´å›åˆ"""
        self.current_episode += 1
        self.episode_rewards = []
        self.episode_start_time = time.time()
        self.episode_exploration_count = 0
        self.episode_exploitation_count = 0

    def end_episode(self):
        """çµæŸç•¶å‰è¨“ç·´å›åˆä¸¦è¨˜éŒ„çµ±è¨ˆ"""
        if not self.episode_rewards:
            return

        total_reward = sum(self.episode_rewards)
        avg_reward = total_reward / len(self.episode_rewards)
        episode_duration = time.time() - self.episode_start_time

        # è¨˜éŒ„è¨“ç·´æ­·å²
        self.training_history['episodes'].append(self.current_episode)
        self.training_history['rewards'].append(total_reward)
        self.training_history['avg_rewards'].append(avg_reward)
        self.training_history['epsilons'].append(self.epsilon)
        self.training_history['q_table_sizes'].append(len(self.q_table))
        self.training_history['timestamps'].append(datetime.now())
        self.training_history['losses'].append(0)  # å¯ä»¥æ ¹æ“šéœ€è¦è¨ˆç®—æå¤±

        # è¨ˆç®—æ¢ç´¢ç‡
        total_choices = self.episode_exploration_count + self.episode_exploitation_count
        if total_choices > 0:
            exploration_rate = (self.episode_exploration_count / total_choices) * 100
        else:
            exploration_rate = 0

        self.training_history['exploration_rates'].append(exploration_rate)

        # è¨˜éŒ„å‹•ä½œåˆ†ä½ˆ
        action_dist = self.exploration_stats['action_distribution'].copy()
        self.training_history['action_distributions'].append(action_dist)

    def update(self, state, action, reward, next_state, next_action=None):
            """
            æ›´æ–° Q-table ä¸¦è¨˜éŒ„è¨“ç·´æ•¸æ“š
            """
            # è¨˜éŒ„çå‹µ
            self.episode_rewards.append(reward)

            # è¨˜éŒ„æ¢ç´¢/åˆ©ç”¨
            discrete_state = self._discretize_state(state)
            q_values = self.q_table[discrete_state]
            if np.random.random() < self.epsilon:
                self.episode_exploration_count += 1
            else:
                self.episode_exploitation_count += 1

            # åŸæœ‰çš„æ›´æ–°é‚è¼¯
            discrete_next_state = self._discretize_state(next_state)
            current_q = self.q_table[discrete_state][action]

            if next_action is not None:
                next_q = self.q_table[discrete_next_state][next_action]
            else:
                next_q = np.max(self.q_table[discrete_next_state])

            new_q = current_q + self.learning_rate * (
                    reward + self.gamma * next_q - current_q
            )

            self.q_table[discrete_state][action] = new_q

            # è¡°æ¸›æ¢ç´¢ç‡
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay

    def check_training_process(self, detailed=False, plot=False):
        """
        æª¢æŸ¥è¨“ç·´éç¨‹ç‹€æ…‹

        Args:
            detailed: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
            plot: æ˜¯å¦ç”Ÿæˆè¨“ç·´æ›²ç·šåœ–

        Returns:
            è¨“ç·´éç¨‹çµ±è¨ˆå­—å…¸
        """
        if not self.training_history['episodes']:
            return {"status": "å°šæœªé–‹å§‹è¨“ç·´"}

        current_episode = self.current_episode
        latest_reward = self.training_history['rewards'][-1] if self.training_history['rewards'] else 0
        avg_reward = self.training_history['avg_rewards'][-1] if self.training_history['avg_rewards'] else 0

        stats = {
            'current_episode': current_episode,
            'total_episodes': len(self.training_history['episodes']),
            'latest_reward': round(latest_reward, 2),
            'latest_avg_reward': round(avg_reward, 2),
            'current_epsilon': round(self.epsilon, 4),
            'q_table_size': len(self.q_table),
            'training_duration': self._get_training_duration(),
            'overall_avg_reward': round(np.mean(self.training_history['rewards']), 2) if self.training_history[
                'rewards'] else 0,
            'best_reward': round(max(self.training_history['rewards']), 2) if self.training_history[
                'rewards'] else 0,
            'convergence_status': self._check_convergence()
        }

        if detailed:
            # æ·»åŠ è©³ç´°çµ±è¨ˆ
            stats.update({
                'reward_trend': self._get_reward_trend(),
                'exploration_trend': self._get_exploration_trend(),
                'recent_performance': self._get_recent_performance(10),
                'action_distribution': self._get_current_action_distribution()
            })

        if plot:
            self._plot_training_progress()

        return stats

    def _get_training_duration(self):
        """è¨ˆç®—ç¸½è¨“ç·´æ™‚é–“"""
        if not self.training_history['timestamps']:
            return "0s"

        start_time = self.training_history['timestamps'][0]
        end_time = self.training_history['timestamps'][-1]
        duration = (end_time - start_time).total_seconds()

        if duration < 60:
            return f"{int(duration)}s"
        elif duration < 3600:
            return f"{int(duration // 60)}m {int(duration % 60)}s"
        else:
            return f"{int(duration // 3600)}h {int((duration % 3600) // 60)}m"

    def _check_convergence(self):
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¶æ–‚"""
        if len(self.training_history['rewards']) < 20:
            return "éœ€è¦æ›´å¤šè¨“ç·´æ•¸æ“š"

        recent_rewards = self.training_history['rewards'][-10:]
        reward_std = np.std(recent_rewards)

        if reward_std < 5:  # çå‹µæ³¢å‹•å¾ˆå°
            return "å¯èƒ½å·²æ”¶æ–‚"
        elif self.epsilon <= self.epsilon_min + 0.01:
            return "æ¢ç´¢ç‡å·²ç©©å®š"
        else:
            return "ä»åœ¨å­¸ç¿’ä¸­"

    def _get_reward_trend(self):
        """ç²å–çå‹µè¶¨å‹¢"""
        if len(self.training_history['rewards']) < 5:
            return "æ•¸æ“šä¸è¶³"

        recent_rewards = self.training_history['rewards'][-5:]
        if all(recent_rewards[i] <= recent_rewards[i + 1] for i in range(len(recent_rewards) - 1)):
            return "ä¸Šå‡"
        elif all(recent_rewards[i] >= recent_rewards[i + 1] for i in range(len(recent_rewards) - 1)):
            return "ä¸‹é™"
        else:
            return "æ³¢å‹•"

    def _get_exploration_trend(self):
            """ç²å–æ¢ç´¢è¶¨å‹¢"""
            if len(self.training_history['exploration_rates']) < 5:
                return "æ•¸æ“šä¸è¶³"

            recent_rates = self.training_history['exploration_rates'][-5:]
            return round(np.mean(recent_rates), 2)

    def _get_recent_performance(self, n=10):
        """ç²å–æœ€è¿‘nå€‹å›åˆçš„æ€§èƒ½"""
        if len(self.training_history['rewards']) < n:
            n = len(self.training_history['rewards'])

        recent_rewards = self.training_history['rewards'][-n:]
        return {
            'avg_reward': round(np.mean(recent_rewards), 2),
            'min_reward': round(min(recent_rewards), 2),
            'max_reward': round(max(recent_rewards), 2),
            'std_reward': round(np.std(recent_rewards), 2)
        }

    def _get_current_action_distribution(self):
        """ç²å–ç•¶å‰å‹•ä½œåˆ†ä½ˆ"""
        total = sum(self.exploration_stats['action_distribution'])
        if total == 0:
            return {f'action_{i}': 0 for i in range(self.action_size)}

        return {
            f'action_{i}': round((count / total) * 100, 2)
            for i, count in enumerate(self.exploration_stats['action_distribution'])
        }

    def _plot_training_progress(self):
        """ç¹ªè£½è¨“ç·´é€²åº¦åœ–"""
        if len(self.training_history['episodes']) < 2:
            print("éœ€è¦æ›´å¤šæ•¸æ“šä¾†ç¹ªåœ–")
            return

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # çå‹µæ›²ç·š
        ax1.plot(self.training_history['episodes'], self.training_history['rewards'])
        ax1.set_title('Total Reward per Episode')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Reward')
        ax1.grid(True)

        # å¹³å‡çå‹µæ›²ç·š
        ax2.plot(self.training_history['episodes'], self.training_history['avg_rewards'])
        ax2.set_title('Average Reward per Episode')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Average Reward')
        ax2.grid(True)

        # æ¢ç´¢ç‡æ›²ç·š
        ax3.plot(self.training_history['episodes'], self.training_history['exploration_rates'])
        ax3.set_title('Exploration Rate')
        ax3.set_xlabel('Episode')
        ax3.set_ylabel('Exploration Rate (%)')
        ax3.grid(True)

        # Q-table å¤§å°æ›²ç·š
        ax4.plot(self.training_history['episodes'], self.training_history['q_table_sizes'])
        ax4.set_title('Q-table Size')
        ax4.set_xlabel('Episode')
        ax4.set_ylabel('Number of States')
        ax4.grid(True)

        plt.tight_layout()
        plt.savefig('training_progress.png')
        plt.close()

        print("è¨“ç·´é€²åº¦åœ–å·²ä¿å­˜ç‚º 'training_progress.png'")

    def get_training_summary(self):
            """ç²å–è¨“ç·´æ‘˜è¦å ±å‘Š"""
            stats = self.check_training_process(detailed=True)

            summary = f"""
            ğŸ¯ è¨“ç·´éç¨‹æ‘˜è¦
            ========================
            ç¸½å›åˆæ•¸: {stats['total_episodes']}
            ç•¶å‰å›åˆ: {stats['current_episode']}
            è¨“ç·´æ™‚é•·: {stats['training_duration']}

            ğŸ“Š çå‹µçµ±è¨ˆ:
            æœ€æ–°çå‹µ: {stats['latest_reward']}
            å¹³å‡çå‹µ: {stats['overall_avg_reward']}
            æœ€ä½³çå‹µ: {stats['best_reward']}
            çå‹µè¶¨å‹¢: {stats['reward_trend']}

            âš™ï¸ æ¨¡å‹ç‹€æ…‹:
            Q-table å¤§å°: {stats['q_table_size']} å€‹ç‹€æ…‹
            ç•¶å‰æ¢ç´¢ç‡: {stats['current_epsilon']}
            æ¢ç´¢è¶¨å‹¢: {stats['exploration_trend']}%
            æ”¶æ–‚ç‹€æ…‹: {stats['convergence_status']}

            ğŸ¯ æœ€è¿‘è¡¨ç¾:
            å¹³å‡çå‹µ: {stats['recent_performance']['avg_reward']}
            æ³¢å‹•ç¯„åœ: Â±{stats['recent_performance']['std_reward']}
            """

            return summary




    def choose_action(self, state):
        """
        æ ¹æ“š epsilon-greedy ç­–ç•¥é¸æ“‡å‹•ä½œ
        """
        discrete_state = self._discretize_state(state)

        self.exploration_stats['total_choices'] += 1

        if np.random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéš¨æ©Ÿé¸æ“‡å‹•ä½œ
            action = np.random.randint(self.action_size)
            self.exploration_stats['exploration_choices'] += 1
            self.exploration_stats['last_choice_type'] = 'exploration'
        else:
            # åˆ©ç”¨ï¼šé¸æ“‡Qå€¼æœ€å¤§çš„å‹•ä½œ
            q_values = self.q_table[discrete_state]
            action = np.argmax(q_values)
            self.exploration_stats['exploitation_choices'] += 1
            self.exploration_stats['last_choice_type'] = 'exploitation'

        # è¨˜éŒ„å‹•ä½œåˆ†ä½ˆ
        self.exploration_stats['action_distribution'][action] += 1

        return action

    def check_exploration(self, detailed=False):
        """
        æª¢æŸ¥æ¢ç´¢ç‹€æ…‹å’Œçµ±è¨ˆä¿¡æ¯

        Args:
            detailed: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯

        Returns:
            æ¢ç´¢ç‹€æ…‹çµ±è¨ˆå­—å…¸
        """
        total = self.exploration_stats['total_choices']
        if total == 0:
            return {"status": "å°šæœªé€²è¡Œä»»ä½•é¸æ“‡"}

        exploration_rate = (self.exploration_stats['exploration_choices'] / total) * 100
        exploitation_rate = (self.exploration_stats['exploitation_choices'] / total) * 100

        stats = {
            'total_choices': total,
            'exploration_choices': self.exploration_stats['exploration_choices'],
            'exploitation_choices': self.exploration_stats['exploitation_choices'],
            'exploration_rate_percent': round(exploration_rate, 2),
            'exploitation_rate_percent': round(exploitation_rate, 2),
            'current_epsilon': round(self.epsilon, 4),
            'epsilon_min': self.epsilon_min,
            'last_choice_type': self.exploration_stats['last_choice_type'],
            'is_exploring': exploration_rate > 20  # å¦‚æœæ¢ç´¢ç‡å¤§æ–¼20%ï¼Œèªç‚ºé‚„åœ¨æ¢ç´¢éšæ®µ
        }

        if detailed:
            # æ·»åŠ è©³ç´°çš„å‹•ä½œåˆ†ä½ˆä¿¡æ¯
            action_distribution = self.exploration_stats['action_distribution']
            action_percentages = (action_distribution / total) * 100

            stats['action_distribution'] = {
                f'action_{i}': {
                    'count': int(action_distribution[i]),
                    'percentage': round(action_percentages[i], 2)
                } for i in range(self.action_size)
            }

            # æ·»åŠ Q-tableçµ±è¨ˆ
            stats['q_table_size'] = len(self.q_table)
            stats['average_q_values'] = self._get_average_q_values()

        return stats

    def _get_average_q_values(self):
        """è¨ˆç®—æ‰€æœ‰ç‹€æ…‹çš„å¹³å‡Qå€¼"""
        if not self.q_table:
            return {f'action_{i}': 0 for i in range(self.action_size)}

        total_q = np.zeros(self.action_size)
        count = 0

        for state_q in self.q_table.values():
            total_q += state_q
            count += 1

        return {f'action_{i}': round(total_q[i] / count, 4) for i in range(self.action_size)}

    def reset_exploration_stats(self):
        """é‡ç½®æ¢ç´¢çµ±è¨ˆ"""
        self.exploration_stats = {
            'total_choices': 0,
            'exploration_choices': 0,
            'exploitation_choices': 0,
            'action_distribution': np.zeros(self.action_size),
            'last_choice_type': None
        }
        print("æ¢ç´¢çµ±è¨ˆå·²é‡ç½®")

    def set_epsilon(self, new_epsilon):
        """æ‰‹å‹•è¨­ç½®epsilonå€¼"""
        old_epsilon = self.epsilon
        self.epsilon = max(self.epsilon_min, min(new_epsilon, 1.0))
        print(f"Epsilon å¾ {old_epsilon} èª¿æ•´ç‚º {self.epsilon}")

    def get_exploration_recommendation(self):
            """
            æ ¹æ“šç•¶å‰ç‹€æ…‹çµ¦å‡ºæ¢ç´¢å»ºè­°
            """
            stats = self.check_exploration()

            if stats['total_choices'] < 50:
                return "è¨“ç·´åˆæœŸï¼Œå»ºè­°ä¿æŒè¼ƒé«˜æ¢ç´¢ç‡"

            if stats['exploration_rate_percent'] < 5:
                return "æ¢ç´¢ç‡éä½ï¼Œå»ºè­°é©ç•¶å¢åŠ æ¢ç´¢"
            elif stats['exploration_rate_percent'] > 40:
                return "æ¢ç´¢ç‡éé«˜ï¼Œå»ºè­°æ¸›å°‘æ¢ç´¢ï¼Œå¢åŠ åˆ©ç”¨"
            else:
                return "æ¢ç´¢ç‡åœ¨åˆç†ç¯„åœå…§"




    def _load_best_params(self):
        """å¾æ–‡ä»¶åŠ è¼‰æœ€ä½³åƒæ•¸"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r') as f:
                    params = json.load(f)
                print(f"âœ… å·²åŠ è¼‰æœ€ä½³åƒæ•¸: {params}")
                return params
            return None
        except Exception as e:
            print(f"âŒ åŠ è¼‰åƒæ•¸å¤±æ•—: {e}")
            return None

    def _get_training_params(self):
        """ç²å–è¨“ç·´åƒæ•¸"""
        if self.best_params:
            return self.best_params
        else:
            return {
                'learning_rate': 0.1,
                'gamma': 0.9,
                'initial_epsilon': 0.3
            }

    def save_best_params(self, params):
        """ä¿å­˜æœ€ä½³åƒæ•¸åˆ°æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w') as f:
                json.dump(params, f, indent=4)
            print(f"âœ… æœ€ä½³åƒæ•¸å·²ä¿å­˜åˆ° {self.config_file}")
            self.best_params = params
        except Exception as e:
            print(f"âŒ ä¿å­˜åƒæ•¸å¤±æ•—: {e}")

    def _discretize_state0(self, state):
        """
        é›¢æ•£åŒ–ç‹€æ…‹
        """
        if isinstance(state, (list, tuple, np.ndarray)):
            if len(state) == 0:
                return (0,)

            # è™•ç†å¤šç¶­ç‹€æ…‹
            if isinstance(state[0], (list, tuple, np.ndarray)):
                # å¦‚æœæ˜¯äºŒç¶­æ•¸çµ„ï¼Œå±•å¹³ä¸¦é›¢æ•£åŒ–
                flattened = np.array(state).flatten()
                discrete_tuple = tuple(int(np.clip(x * 100, 0, 99)) for x in flattened[:3])  # é™åˆ¶ç‰¹å¾µæ•¸é‡
            else:
                # ä¸€ç¶­æ•¸çµ„
                discrete_tuple = tuple(int(np.clip(x * 100, 0, 99)) for x in state[:3])  # é™åˆ¶ç‰¹å¾µæ•¸é‡
            return discrete_tuple
        else:
            # å–®ä¸€æ•¸å€¼
            return (int(np.clip(state * 100, 0, 99)),)



    def get_q_values(self, state):
        """ç²å–æŒ‡å®šç‹€æ…‹çš„Qå€¼"""
        discrete_state = self._discretize_state(state)
        return self.q_table[discrete_state].copy()

    def get_policy(self, state):
        """ç²å–æŒ‡å®šç‹€æ…‹çš„ç­–ç•¥ï¼ˆå‹•ä½œæ¦‚ç‡åˆ†ä½ˆï¼‰"""
        q_values = self.get_q_values(state)
        # ä½¿ç”¨softmaxå°‡Qå€¼è½‰æ›ç‚ºæ¦‚ç‡åˆ†ä½ˆ
        exp_q = np.exp(q_values - np.max(q_values))  # æ•¸å€¼ç©©å®šæ€§
        return exp_q / np.sum(exp_q)

    def reset_epsilon(self, new_epsilon=None):
        """é‡ç½®æ¢ç´¢ç‡"""
        if new_epsilon is not None:
            self.epsilon = new_epsilon
        else:
            params = self._get_training_params()
            self.epsilon = params.get('initial_epsilon', 0.3)

    def get_stats(self):
        """ç²å–æ¨¡å‹çµ±è¨ˆä¿¡æ¯"""
        return {
            'q_table_size': len(self.q_table),
            'epsilon': self.epsilon,
            'learning_rate': self.learning_rate,
            'gamma': self.gamma,
            'action_size': self.action_size
        }

    def debug_state(self, state):
        """èª¿è©¦ç‹€æ…‹é›¢æ•£åŒ–"""
        discrete_state = self._discretize_state(state)
        q_values = self.q_table[discrete_state]
        return {
            'original_state': state,
            'discrete_state': discrete_state,
            'q_values': q_values,
            'recommended_action': np.argmax(q_values)
        }


    def _discretize_state(self, state):
        discrete = 0
        if isinstance(state, (list, tuple, np.ndarray)):
            # å¦‚æœç‹€æ…‹æ˜¯æ•¸çµ„ï¼Œè™•ç†æ¯å€‹å…ƒç´ 
            for i, value in enumerate(state):
                if isinstance(value, (list, tuple, np.ndarray)):
                    # å¦‚æœå…ƒç´ æœ¬èº«ä¹Ÿæ˜¯æ•¸çµ„ï¼Œéæ­¸è™•ç†æˆ–ç‰¹æ®Šè™•ç†
                    discrete += int(np.mean(value) * 100) * (10 ** (i * 2))
                else:
                    # å–®ä¸€æ•¸å€¼
                    discrete += int(value * 100) * (10 ** (i * 2))
        else:
            # å–®ä¸€æ•¸å€¼ç‹€æ…‹
            discrete = int(state * 100)

        return discrete


    # ---------- Qç½‘ç»œ ----------
    def build_q_network(self, latent_dim, num_actions):
        inp = layers.Input(shape=(latent_dim,), name="latent_in")
        x = inp
        for h in Q_HIDDEN:
            x = layers.Dense(h, activation="relu")(x)
        out = layers.Dense(num_actions, activation=None, name="q_out")(x)
        return models.Model(inp, out, name="q_net")

    # ---------- å·¥å…· ----------
    @staticmethod
    def action_int_to_bits(a_int):
        return np.array([(a_int >> i) & 1 for i in range(NUM_SWITCH)], dtype=np.float32)

    @staticmethod
    def select_action_batch(q_values, eps):
        # q_values: (B, A)
        if q_values.shape[0] == 0:
            return np.zeros((0,), dtype=np.int32)
        greedy = np.argmax(q_values, axis=1).astype(np.int32)
        rand_mask = (np.random.rand(q_values.shape[0]) < eps)
        random_actions = np.random.randint(q_values.shape[1], size=q_values.shape[0])
        actions = np.where(rand_mask, random_actions, greedy).astype(np.int32)
        return actions

    @staticmethod
    def compute_reward_batch(labels, actions_bits):
        """
        labels: (B,) å€¼åŸŸç¤ºä¾‹ {0,1,å…¶ä»–}
        actions_bits: (B, NUM_SWITCH)
        """
        # ç¤ºä¾‹å¥–åŠ±ï¼š0 -> +1, 1 -> -1, else -> -2
        rewards = np.where(labels == 0, 1.0, np.where(labels == 1, -1.0, -2.0))
        rewards = rewards - ACTION_COST * np.sum(actions_bits, axis=1)
        return rewards.astype(np.float32)

    def encode_sequence(self, encoder, X_seq):
        """
        X_seq: (B, seq_len, F) æˆ– (1, seq_len, F)
        è¿”å› (B, latent_dim)
        """
        return encoder(X_seq).numpy().astype(np.float32)

    def encode_per_timestep_aug(self, encoder, raw, seq_len=None, step_size=1):
        """
        Args:
            encoder: æ¨¡å‹ (è¼¸å…¥ [B, seq_len, F] -> è¼¸å‡º latent)
            raw: numpy array, shape [T, F]
            seq_len: æ¯å€‹çª—å£é•·åº¦ (int æˆ– None)ï¼ŒNone æ™‚è‡ªå‹•æ±ºå®š
            step_size: æ»‘å‹•æ­¥é•·

        Returns:
            s_latent_seq: shape [N, D]ï¼Œæ‰€æœ‰çª—å£ latent
            starts: æ¯å€‹çª—å£å°æ‡‰çš„èµ·å§‹ç´¢å¼•
        """
        T = raw.shape[0]

        # --- å‹•æ…‹æ±ºå®šçª—å£é•·åº¦ ---
        if seq_len is None:
            # ä¾‹å¦‚å– T çš„ 1/10ï¼Œè‡³å°‘ 5
            seq_len = max(10, T // 20)

        # --- æ»‘å‹•çª—å£åˆ‡ç‰‡ ---
        starts = list(range(0, T - seq_len + 1, step_size))
        seqs = [raw[i:i + seq_len] for i in starts]

        if not seqs:
            return np.zeros((0, encoder.output_shape[-1])), []

        seqs = np.array(seqs)  # [N, seq_len, F]

        # --- ç·¨ç¢¼ ---
        s_latent_seq = encoder.predict(seqs, verbose=0)  # [N, D]

        return s_latent_seq, starts


    @tf.function
    def _sarsa_batch_update_tf(self, s_latent, actions, s_next_latent, rewards):
        # s_latent: (B, D), actions: (B,), s_next_latent: (B, D), rewards: (B,)
        with tf.GradientTape() as tape:
            q_pred_all = self.q_net(s_latent, training=True)                 # (B, A)
            batch_idx = tf.range(tf.shape(actions)[0], dtype=tf.int32)
            q_pred = tf.gather_nd(q_pred_all, tf.stack([batch_idx, actions], axis=1))  # (B,)

            # target ä½¿ç”¨ next state's max-Qï¼ˆSARSAå¯æ”¹ï¼šç”¨ next action çš„ Qï¼›æ­¤å¤„åš Double-checkï¼‰
            q_next_all = self.q_net(s_next_latent, training=False)           # (B, A)
            next_actions = tf.argmax(q_next_all, axis=1, output_type=tf.int32)
            q_next = tf.gather_nd(q_next_all, tf.stack([batch_idx, next_actions], axis=1))  # (B,)

            rewards = tf.cast(rewards, q_next.dtype)
            target = rewards + GAMMA * q_next                                # (B,)

            loss = tf.reduce_mean(tf.square(target - q_pred))
        grads = tape.gradient(loss, self.q_net.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.q_net.trainable_variables))
        return loss

    def sarsa_full_batch_robust(self, encoder, seq_len, step_size=1):
        """
        æ¯æ¬¡ä» CSV é‡‡æ ·ä¸€æ®µï¼Œæ„é€  (s_t, a_t, r_{t+1}, s_{t+1}) çš„å…¨æ‰¹é‡æ ·æœ¬å¹¶æ›´æ–°ã€‚
        """
        for ep in range(1, SARSA_EPISODES + 1):
            df = pd.read_csv(np.random.choice(files))

            raw = df[["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values.astype(np.float32)
            labels = df["label"].values.astype(np.int32)

            # å½’ä¸€åŒ–éæ§åˆ¶é‡
            raw[:, 0] /= 40.0
            raw[:, 1] /= 100.0
            raw[:, 2] /= 1000.0

            # è®¡ç®—æ‰€æœ‰çª—å£ latent
            s_latent_seq, starts = self.encode_per_timestep(encoder, raw)  # (N, D)
            N = s_latent_seq.shape[0]
            if N <= 1:
                # ä¸è¶³ä»¥å½¢æˆ (s_t, s_{t+1})
                continue

            # é¢„æµ‹å½“å‰ Q(s_t, Â·)
            q_pred = self.q_net.predict(s_latent_seq[:-1], verbose=0)  # (N-1, A)
            # æ¢ç´¢å™ªå£°ï¼ˆå¾ˆå°ï¼‰
            q_pred = q_pred + np.random.randn(*q_pred.shape).astype(np.float32) * 0.01

            # Îµ-greedy é€‰è¡ŒåŠ¨
            actions = self.select_action_batch(q_pred, self.eps)              # (N-1,)
            actions_bits = np.stack([self.action_int_to_bits(a) for a in actions], axis=0)  # (N-1, NUM_SWITCH)

            # å¥–åŠ±ï¼šä½¿ç”¨ä¸‹ä¸€æ­¥çª—å£çš„æ ‡ç­¾ï¼ˆä¸ s_{t+1} å¯¹é½ï¼‰
            label_next_idx = [min(s + seq_len, len(labels) - 1) for s in starts[:-1]]
            rewards = self.compute_reward_batch(labels=np.array(label_next_idx, dtype=np.int32)*0 + labels[label_next_idx],
                                                actions_bits=actions_bits)

            # æ„é€  s_next
            s_next_latent_seq = s_latent_seq[1:]                              # (N-1, D)

            # åˆ†æ‰¹æ›´æ–°ï¼Œé¿å…è¿‡å¤§ batch
            total_loss = 0.0
            B = BATCH_MAX
            for i in range(0, len(actions), B):
                j = min(i + B, len(actions))
                if j - i <= 0: continue
                loss = self._sarsa_batch_update_tf(
                    tf.convert_to_tensor(s_latent_seq[i:j], dtype=tf.float32),
                    tf.convert_to_tensor(actions[i:j], dtype=tf.int32),
                    tf.convert_to_tensor(s_next_latent_seq[i:j], dtype=tf.float32),
                    tf.convert_to_tensor(rewards[i:j], dtype=tf.float32),
                )
                total_loss += float(loss.numpy())

            # è¡°å‡ epsilon
            self.eps = max(EPS_END, self.eps * EPS_DECAY)

            if ep % 10 == 0 or ep == 1:
                print(f"Episode {ep:03d}/{SARSA_EPISODES} | eps={self.eps:.3f} | lossâ‰ˆ{total_loss:.4f} | samples={len(actions)}")

    def rollout_meta_sarsa(self, X_train, X_val, encoder, seq_len, steps=30, step_size=1, epsilon=0.1):
        """
        ä»…ç”¨äºå¿«é€Ÿ sanity checkï¼šåŸºäºç¼–ç çš„ Îµ-è´ªå©ª rolloutã€‚
        """
        latents, starts = self.encode_per_timestep(encoder, X_train, seq_len=seq_len, step_size=step_size)
        if latents.shape[0] == 0:
            print("No latent windows; check seq_len/step_size.")
            return

        rewards_list, actions_list = [], []
        for t in range(min(steps, latents.shape[0])):
            q_vals = self.q_net.predict(latents[t:t+1], verbose=0)[0]
            q_vals = q_vals + np.random.randn(NUM_ACTIONS).astype(np.float32) * 0.01
            if np.random.rand() < epsilon:
                a = np.random.randint(NUM_ACTIONS)
            else:
                a = int(np.argmax(q_vals))
            bits = self.action_int_to_bits(a)
            actions_list.append(bits)

            if X_val is not None and len(X_val) > 0:
                idx = min(starts[t] + seq_len, len(X_val) - 1)
                r = self.compute_reward_batch(np.array([int(X_val[idx, 0]*0)]),  # è¿™é‡Œæ²¡æœ‰çœŸå®æ ‡ç­¾ï¼Œä»…æ¼”ç¤ºï¼›å»ºè®®æ¢æˆ df çš„ label
                                              np.array([bits]))[0]
            else:
                r = 0.0
            rewards_list.append(r)

        print("Actions:", [a.tolist() for a in actions_list])
        print("Rewards:", rewards_list)
        # åˆ›å»ºDataFrame
        df = pd.DataFrame({
            'Action_0': [action[0] for action in actions_list],
            'Action_1': [action[1] for action in actions_list],
            'Action_2': [action[2] for action in actions_list],
            'Action_3': [action[3] for action in actions_list],
            'Reward': rewards_list
        })

        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        df.to_csv('actions_rewards.csv', index=False)
        print("æ•°æ®å·²ä¿å­˜åˆ° actions_rewards.csv")


# ============== æµ‹è¯• rollout ==============
def test_rollout(qmodel, encoder, steps=30, save_path="rollout_results.csv"):
    df = pd.read_csv(np.random.choice(files))

    # è·å–å‰10ä¸ªæ—¶é—´æ­¥çš„æ•°æ®ä½œä¸ºåˆå§‹çŠ¶æ€
    initial_data = []
    for i in range(10):
        row = df.iloc[i][["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values.astype(np.float32)
        initial_data.append(normalize_row(row))

    # åˆ›å»ºæ»‘åŠ¨çª—å£ç¼“å†²åŒº
    state_buffer = np.array(initial_data)  # (10, 7)

    print("\nTest rollout:")

    # åˆ›å»ºç»“æœåˆ—è¡¨
    results = []

    for t in range(steps):
        # ä½¿ç”¨æœ€è¿‘10ä¸ªæ—¶é—´æ­¥çš„æ•°æ®ä½œä¸ºè¾“å…¥
        s_latent = encoder(state_buffer[np.newaxis, :, :]).numpy()[0]  # (latent_dim,)

        qv = qmodel.q_net.predict(s_latent.reshape(1, -1), verbose=0)[0]
        a = int(np.argmax(qv))
        bits = qmodel.action_int_to_bits(a)

        # è·å–ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„æ•°æ®
        next_idx = min(t + 10, len(df) - 1)
        next_row = df.iloc[next_idx][["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values.astype(
            np.float32)
        next_row = normalize_row(next_row)
        next_row[3:3 + NUM_SWITCH] = bits  # ç”¨åŠ¨ä½œæ›¿æ¢å¼€å…³çŠ¶æ€

        # æ›´æ–°çŠ¶æ€ç¼“å†²åŒºï¼šç§»é™¤æœ€æ—§çš„æ•°æ®ï¼Œæ·»åŠ æœ€æ–°çš„æ•°æ®
        state_buffer = np.vstack([state_buffer[1:], next_row[np.newaxis, :]])

        label_next = int(df.iloc[next_idx]["label"]) if "label" in df.columns else 0
        r = qmodel.compute_reward_batch(np.array([label_next]), np.array([bits]))[0]

        # æ‰“å°ç»“æœ
        print(f"t={t:02d} action={a:02d} bits={bits.tolist()} reward={r:.3f} label_next={label_next}")

        # ä¿å­˜ç»“æœåˆ°åˆ—è¡¨
        results.append({
            'time_step': t,
            'action': a,
            'ac': bits[0],
            'heater': bits[1],
            'dehum': bits[2],
            'hum': bits[3],
            'reward': r,
            'label_next': label_next,
            'q_value_max': np.max(qv),
            'temp': next_row[0],
            'humid': next_row[1],
            'light': next_row[2]
        })

    # å°†ç»“æœè½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜ä¸ºCSV
    results_df = pd.DataFrame(results)
    results_df.to_csv(save_path, index=False)
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {save_path}")

    return results_df


def test_rollout_detailed(qmodel, encoder, steps=30, save_path="rollout_detailed_results.csv"):
    df = pd.read_csv(np.random.choice(files))

    # è·å–å‰10ä¸ªæ—¶é—´æ­¥çš„æ•°æ®ä½œä¸ºåˆå§‹çŠ¶æ€
    initial_data = []
    for i in range(10):
        row = df.iloc[i][["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values.astype(np.float32)
        initial_data.append(normalize_row(row))

    state_buffer = np.array(initial_data)

    print("\nTest rollout (è¯¦ç»†ç‰ˆ):")

    results = []

    for t in range(steps):
        s_latent = encoder(state_buffer[np.newaxis, :, :]).numpy()[0]
        qv = qmodel.q_net.predict(s_latent.reshape(1, -1), verbose=0)[0]
        a = int(np.argmax(qv))
        bits = qmodel.action_int_to_bits(a)

        next_idx = min(t + 10, len(df) - 1)
        next_row = df.iloc[next_idx][["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values.astype(
            np.float32)
        original_next_row = next_row.copy()  # ä¿å­˜åŸå§‹å€¼
        next_row = normalize_row(next_row)
        next_row[3:3 + NUM_SWITCH] = bits

        state_buffer = np.vstack([state_buffer[1:], next_row[np.newaxis, :]])

        label_next = int(df.iloc[next_idx]["label"]) if "label" in df.columns else 0
        r = qmodel.compute_reward_batch(np.array([label_next]), np.array([bits]))[0]

        print(f"t={t:02d} action={a:02d} bits={bits.tolist()} reward={r:.3f} label_next={label_next}")

        # ä¿å­˜æ›´è¯¦ç»†çš„ä¿¡æ¯
        results.append({
            'time_step': t,
            'action': a,
            'action_binary': f"{a:04b}",
            'ac': bits[0],
            'heater': bits[1],
            'dehum': bits[2],
            'hum': bits[3],
            'reward': r,
            'label_next': label_next,
            'q_value_max': np.max(qv),
            'q_values': qv.tolist(),  # æ‰€æœ‰Qå€¼
            'temp': original_next_row[0],  # åŸå§‹æ¸©åº¦å€¼
            'humid': original_next_row[1],  # åŸå§‹æ¹¿åº¦å€¼
            'light': original_next_row[2],  # åŸå§‹å…‰ç…§å€¼
            'temp_normalized': next_row[0],  # æ ‡å‡†åŒ–æ¸©åº¦
            'humid_normalized': next_row[1],  # æ ‡å‡†åŒ–æ¹¿åº¦
            'light_normalized': next_row[2]  # æ ‡å‡†åŒ–å…‰ç…§
        })

    # ä¿å­˜ä¸ºCSV
    results_df = pd.DataFrame(results)
    results_df.to_csv(save_path, index=False, float_format='%.4f')
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {save_path}")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"å¹³å‡å¥–åŠ±: {results_df['reward'].mean():.3f}")
    print(f"æ€»å¥–åŠ±: {results_df['reward'].sum():.3f}")
    print(f"åŠ¨ä½œåˆ†å¸ƒ:")
    print(results_df['action'].value_counts().sort_index())

    return results_df
# ä½¿ç”¨ç¤ºä¾‹
# results = test_rollout(qmodel, encoder, steps=30, save_path="rollout_results.csv")
# ============== æ•°æ®å·¥å…· ==============
def load_all_csvs(data_dir):
    dfs = [pd.read_csv(os.path.join(data_dir, f)) for f in os.listdir(data_dir) if f.endswith(".csv")]
    return pd.concat(dfs, axis=0).reset_index(drop=True)

def save_sarsa_tflite(q_net):
    converter = tf.lite.TFLiteConverter.from_keras_model(q_net)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()
    with open('../hvac_controller.tflite', 'wb') as f:
        f.write(tflite_model)
    q_net.save("hvac_controller.h5")
    print(f"Model size: {len(tflite_model)} bytes")


# ============== ä¸»ç¨‹åº ==============
def main():
    qmodel = QModel_aug(action_size=NUM_ACTIONS)
    # è®­ç»ƒ
    #qmodel.sarsa_full_batch_robust(encoder, seq_len=SEQ_LEN, step_size=1)
    #save_sarsa_tflite(qmodel.q_net)

    # åŸºæœ¬ç‰ˆæœ¬
    #results = test_rollout(qmodel, encoder, steps=30, save_path="my_rollout_results.csv")

    # è¯¦ç»†ç‰ˆæœ¬
    #detailed_results = test_rollout_detailed(qmodel, encoder, steps=30, save_path="detailed_rollout_results.csv")
    # æˆ–è€…å•ç‹¬è¿è¡ŒæŸäº›è°ƒè¯•åŠŸèƒ½
    #debugger = RL_Debugger()
    from fed_server.meta_sarsa.util_train_plant_env import PlantGrowthEnv
    env=PlantGrowthEnv()
    hyperparameter_tuning(qmodel,   env )
    #run_comprehensive_debug(qmodel, qmodel.encoder, env, files)
    # ä½¿ç”¨ç¤ºä¾‹
    #files = ["your_data_file1.csv", "your_data_file2.csv"]  # ä½ çš„æ–‡ä»¶åˆ—è¡¨
    #debugger = DataDrivenDebugger(files)
    #features, labels = debugger.load_and_preprocess_data()
if __name__ == "__main__":
    main()

