import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime

import tensorflow as tf
from tensorflow.keras import layers, Model

from util_check_estimator import *
from sklearn.preprocessing import StandardScaler
from IPython.display import clear_output
import time


class DataDrivenDebugger:
    def __init__(self, files):
        self.files = files
        self.history = {
            'rewards': [], 'q_values': [], 'actions': [],
            'losses': [], 'epsilons': [], 'correlations': [],
            'states': [], 'predictions': []
        }
        self.scaler = StandardScaler()

    def load_and_preprocess_data(self, file_idx=0):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        df = pd.read_csv(self.files[file_idx])

        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        features = df[["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values
        labels = df["label"].values if "label" in df.columns else np.zeros(len(df))

        # æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾ï¼ˆå‰3ä¸ªï¼‰
        features[:, :3] = self.scaler.fit_transform(features[:, :3])

        return features, labels

    def simulate_environment_step(self, current_idx, features,labels, action_bits):
        """æ¨¡æ‹Ÿç¯å¢ƒæ­¥éª¤ - ä»æ•°æ®ä¸­è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€"""
        next_idx = min(current_idx + 1, len(features) - 1)
        next_state = features[next_idx].copy()

        # ç”¨åŠ¨ä½œæ›¿æ¢å¼€å…³çŠ¶æ€ï¼ˆå4ä¸ªç‰¹å¾ï¼‰
        next_state[3:7] = action_bits

        # è·å–å¥–åŠ±ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰
        reward = self.compute_reward(labels[next_idx] if 'labels' in locals() else 0, action_bits)

        done = (next_idx == len(features) - 1)

        return next_state, reward, done, next_idx

    def compute_reward(self, label, action_bits):
        """è®¡ç®—å¥–åŠ±ï¼ˆæ ¹æ®ä½ çš„å¥–åŠ±å‡½æ•°ï¼‰"""
        # è¿™é‡Œæ˜¯ç¤ºä¾‹å¥–åŠ±å‡½æ•°ï¼Œä½ éœ€è¦æ›¿æ¢ä¸ºä½ çš„å®é™…å®ç°
        base_reward = 1.0 if label == 0 else 0.1  # æ— å¼‚å¸¸æ—¶é«˜å¥–åŠ±

        # æƒ©ç½šè¿‡å¤šçš„è®¾å¤‡å¼€å¯
        device_penalty = -0.1 * sum(action_bits)

        return base_reward + device_penalty




def advanced_diagnostics(qmodel, encoder, rollout_data):
    """
    é«˜çº§è¯Šæ–­å·¥å…·
    """
    print("\n" + "=" * 50)
    print("ğŸ”¬ é«˜çº§è¯Šæ–­åˆ†æ")
    print("=" * 50)

    # 1. åˆ†æBellmanè¯¯å·®
    analyze_bellman_error(qmodel, rollout_data)

    # 2. æ£€æŸ¥å€¼å‡½æ•°ä¼°è®¡
    check_value_estimation(qmodel, encoder)

    # 3. åˆ†ææ¢ç´¢-åˆ©ç”¨å¹³è¡¡
    #analyze_exploration_exploitation(qmodel)

    # 4. æ¢¯åº¦åˆ†æ
    analyze_gradients(qmodel)


def analyze_bellman_error(qmodel, data):
    """åˆ†æBellmanè¯¯å·®"""
    print("\nğŸ“‰ Bellmanè¯¯å·®åˆ†æ:")

    # è®¡ç®—TDè¯¯å·®
    td_errors = []
    for i in range(len(data) - 1):
        state = data.iloc[i][['temp', 'humid', 'light', 'ac', 'heater', 'dehum', 'hum']].values
        next_state = data.iloc[i + 1][['temp', 'humid', 'light', 'ac', 'heater', 'dehum', 'hum']].values
        reward = data.iloc[i + 1]['reward'] if 'reward' in data.columns else 0

        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å…·ä½“å®ç°è®¡ç®—TDè¯¯å·®
        # td_error = ...
        # td_errors.append(td_error)

    if td_errors:
        print(f"  TDè¯¯å·®å‡å€¼: {np.mean(td_errors):.4f}")
        print(f"  TDè¯¯å·®æ ‡å‡†å·®: {np.std(td_errors):.4f}")


def analyze_gradients(qmodel):
    """åˆ†ææ¢¯åº¦ä¿¡æ¯"""
    print("\nğŸ“Š æ¢¯åº¦åˆ†æ:")
    # è¿™é‡Œéœ€è¦è®¿é—®æ¨¡å‹çš„æ¢¯åº¦ä¿¡æ¯
    # å®é™…å®ç°å–å†³äºä½ ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶

    print("  (éœ€è¦æ ¹æ®å…·ä½“æ¡†æ¶å®ç°æ¢¯åº¦ç›‘æ§)")


# 5. å®æ—¶è®­ç»ƒç›‘æ§
def create_training_monitor():
    """åˆ›å»ºå®æ—¶è®­ç»ƒç›‘æ§"""

    class TrainingMonitor:
        def __init__(self):
            self.episode_rewards = []
            self.losses = []
            self.q_values = []

        def update(self, episode_reward, loss, avg_q):
            self.episode_rewards.append(episode_reward)
            self.losses.append(loss)
            self.q_values.append(avg_q)

            if len(self.episode_rewards) % 10 == 0:
                self.plot_progress()

        def plot_progress(self):
            clear_output(wait=True)
            fig, axes = plt.subplots(1, 3, figsize=(15, 4))

            axes[0].plot(self.episode_rewards)
            axes[0].set_title('Episode Rewards')
            axes[0].set_ylabel('Reward')

            axes[1].plot(self.losses)
            axes[1].set_title('Training Loss')
            axes[1].set_ylabel('Loss')
            axes[1].set_yscale('log')

            axes[2].plot(self.q_values)
            axes[2].set_title('Average Q-values')
            axes[2].set_ylabel('Q-value')

            plt.tight_layout()
            plt.show()

    return TrainingMonitor()

class SarsaAgent:
    def __init__(self, state_size, action_size, learning_rate=0.01, gamma=0.99, epsilon=0.1):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate # å±æ€§åæ˜¯ learning_rate
        self.gamma = gamma                 # å±æ€§åæ˜¯ gamma
        self.epsilon = epsilon
        self.q_table = np.zeros((state_size, action_size)) # æˆ–è€…å…¶ä»–çš„Qå‡½æ•°è¡¨ç¤º

    def choose_action(self, state):
        """
        å°æ–¼è¡¨æ ¼æ–¹æ³•ï¼Œéœ€è¦å°‡é€£çºŒç‹€æ…‹é›¢æ•£åŒ–
        """
        if np.random.rand() < self.epsilon:
            return np.random.choice(self.action_size)  # æ¢ç´¢

        else:
            # å°‡é€£çºŒç‹€æ…‹é›¢æ•£åŒ–ç‚ºæ•´æ•¸ç´¢å¼•
            discrete_state = self._discretize_state(state)
            return np.argmax(self.q_table[discrete_state, :])  # åˆ©ç”¨

    def _discretize_state(self, state):
        discrete = 0
        if isinstance(state, (list, tuple, np.ndarray)):
            # å¦‚æœç‹€æ…‹æ˜¯æ•¸çµ„ï¼Œè™•ç†æ¯å€‹å…ƒç´ 
            for i, value in enumerate(state):
                if isinstance(value, (list, tuple, np.ndarray)):
                    # å¦‚æœå…ƒç´ æœ¬èº«ä¹Ÿæ˜¯æ•¸çµ„ï¼Œéæ­¸è™•ç†æˆ–ç‰¹æ®Šè™•ç†
                    discrete += int(np.mean(value) * 100) * (10 ** (i * 2))
                else:
                    # å–®ä¸€æ•¸å€¼
                    discrete += int(value * 100) * (10 ** (i * 2))
        else:
            # å–®ä¸€æ•¸å€¼ç‹€æ…‹
            discrete = int(state * 100)

        return discrete


    def learn(self, state, action, reward, next_state, next_action, done):
        # Sarsa æ›´æ–°å…¬å¼
        current_q = self.q_table[state, action]
        if done:
            target = reward
        else:
            # æ³¨æ„è¿™é‡Œæ˜¯ Q(S', A')ï¼Œæ˜¯ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œâ€œå®é™…é‡‡å–â€çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œ
            target = reward + self.gamma * self.q_table[next_state, next_action]
        # æ›´æ–° Qå€¼
        self.q_table[state, action] += self.learning_rate * (target - current_q)




def hyperparameter_tuning(qmodel, env ):
    """
    æ›´é«˜æ•ˆçš„Sarsaè¶…å‚æ•°è°ƒè¯•
    """
    # æ¸›å°‘åƒæ•¸çµ„åˆï¼Œå…ˆæ¸¬è©¦æœ€é‡è¦çš„åƒæ•¸
    hyperparams_to_test = {
        'learning_rate': [0.1, 0.01],  # å…ˆæ¸¬è©¦å…©å€‹æ¥µç«¯å€¼
        'gamma': [0.9, 0.99],
        'initial_epsilon': [0.3, 0.1],
    }

    best_params = None
    best_performance = -float('inf')

    print("å¼€å§‹Sarsaè¶…å‚æ•°è°ƒè¯•...")

    for lr in hyperparams_to_test['learning_rate']:
        for gamma in hyperparams_to_test['gamma']:
            for initial_eps in hyperparams_to_test['initial_epsilon']:

                print(f"æ¸¬è©¦ LR: {lr}, Gamma: {gamma}, Îµ: {initial_eps}")

                temp_model = qmodel  #SarsaAgent(state_size=latent_dim, action_size=16)
                temp_model.learning_rate = lr
                temp_model.gamma = gamma
                temp_model.epsilon = initial_eps
                # ä½¿ç”¨å›ºå®šè¡°æ¸›å€¼
                temp_model.epsilon_decay = 0.995
                temp_model.epsilon_min = 0.01

                performance = test_hyperparameter_set(temp_model, qmodel.encoder, env, episodes=15)

                if performance > best_performance:
                    best_performance = performance
                    best_params = {
                        'learning_rate': lr,
                        'gamma': gamma,
                        'initial_epsilon': initial_eps
                    }
                    print(f"â†‘ æ–°çš„æœ€ä½³! æ€§èƒ½: {performance:.3f}")

    print(f"\nç¬¬ä¸€éšæ®µæœ€ä½³å‚æ•°: {best_params}")

    # å¯ä»¥åœ¨æ‰¾åˆ°å¤§è‡´ç¯„åœå¾Œï¼Œé€²è¡Œç¬¬äºŒè¼ªæ›´ç²¾ç´°çš„æœç´¢
    return best_params


def test_hyperparameter_set(model, encoder, env, episodes=10):
    """æµ‹è¯•ä¸€ç»„è¶…å‚æ•°çš„æ€§èƒ½"""
    total_reward = 0
    window_size = encoder.input_shape[1]
    feature_keys = ['temp', 'humid', 'light', 'ac', 'heater', 'dehum', 'hum']

    for episode in range(episodes):
        state = env.reset()
        episode_reward = 0
        state_buffer = []

        # é å¡«å……ç·©è¡å€
        for _ in range(window_size):
            # æª¢æŸ¥ state æ˜¯å¦åŒ…å«æ‰€æœ‰éœ€è¦çš„éµ
            if not all(key in state for key in feature_keys):
                missing = [key for key in feature_keys if key not in state]
                print(f"è­¦å‘Š: state ç¼ºå°‘éµ: {missing}")
                # ä½¿ç”¨é»˜èªå€¼å¡«å……
                state_array = np.zeros(len(feature_keys))
            else:
                state_array = np.array([state[key] for key in feature_keys])

            state_buffer.append(state_array)
            next_state, reward, done, _ = env.step(0)
            state = next_state
            if done:
                break

        for step in range(100):
            state_sequence = np.array(state_buffer).reshape(1, window_size, -1)
            state_latent = encoder(state_sequence).numpy()

            action = model.choose_action(state_latent)
            next_state, reward, done, _ = env.step(action)

            # æª¢æŸ¥ next_state æ˜¯å¦åŒ…å«æ‰€æœ‰éœ€è¦çš„éµ
            if not all(key in next_state for key in feature_keys):
                missing = [key for key in feature_keys if key not in next_state]
                print(f"éŒ¯èª¤: next_state ç¼ºå°‘éµ: {missing}")
                # ä½¿ç”¨ç·©è¡å€æœ€å¾Œä¸€å€‹ç‹€æ…‹æˆ–é›¶å¡«å……
                next_state_array = state_buffer[-1] if state_buffer else np.zeros(len(feature_keys))
            else:
                next_state_array = np.array([next_state[key] for key in feature_keys])

            # æ›´æ–°ç·©è¡å€
            state_buffer.pop(0)
            state_buffer.append(next_state_array)

            episode_reward += reward
            state = next_state

            if done:
                break

        total_reward += episode_reward

    return total_reward / episodes



def check_state_representation(encoder):
    """
    æ£€æŸ¥ç¼–ç å™¨çš„çŠ¶æ€è¡¨ç¤º

    å‚æ•°:
        encoder: ç¼–ç å™¨æ¨¡å‹

    è¿”å›:
        dict: åŒ…å«çŠ¶æ€è¡¨ç¤ºæ£€æŸ¥ç»“æœçš„å­—å…¸
    """
    results = {}

    # 1. æ£€æŸ¥æ¨¡å‹å‚æ•°
    results['parameter_check'] = {
        'total_parameters': encoder.count_params(),
        'trainable_parameters': sum([tf.keras.backend.count_params(w) for w in encoder.trainable_weights]),
    }

    # 2. åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size, seq_len, input_dim = 5, 10, 32
    test_input = tf.random.normal((batch_size, seq_len, input_dim))

    # 3. å‰å‘ä¼ æ’­è·å–çŠ¶æ€è¡¨ç¤º
    try:
        # å°è¯•è·å–çŠ¶æ€è¡¨ç¤º
        state_representation = encoder(test_input, training=False)
    except Exception as e:
        results['error'] = f"å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}"
        return results

    # 4. åˆ†æçŠ¶æ€è¡¨ç¤º
    results['state_analysis'] = {
        'shape': state_representation.shape.as_list(),
        'mean': float(tf.reduce_mean(state_representation).numpy()),
        'std': float(tf.math.reduce_std(state_representation).numpy()),
        'min': float(tf.reduce_min(state_representation).numpy()),
        'max': float(tf.reduce_max(state_representation).numpy()),
        'has_nan': tf.reduce_any(tf.math.is_nan(state_representation)).numpy(),
        'has_inf': tf.reduce_any(tf.math.is_inf(state_representation)).numpy(),
    }

    # 5. æ£€æŸ¥çŠ¶æ€è¡¨ç¤ºçš„å¤šæ ·æ€§ï¼ˆé¿å…æ¨¡å¼å´©æºƒï¼‰
    if len(state_representation.shape) > 1:
        # è®¡ç®—ä¸åŒæ ·æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        flattened_states = tf.reshape(state_representation, [state_representation.shape[0], -1])
        norms = tf.norm(flattened_states, axis=1, keepdims=True)
        normalized_states = flattened_states / norms
        cosine_similarity = tf.matmul(normalized_states, normalized_states, transpose_b=True)

        # æ’é™¤å¯¹è§’çº¿å…ƒç´ ï¼ˆè‡ªç›¸ä¼¼æ€§ï¼‰
        mask = 1 - tf.eye(cosine_similarity.shape[0], dtype=cosine_similarity.dtype)
        avg_similarity = tf.reduce_sum(cosine_similarity * mask) / tf.reduce_sum(mask)

        results['diversity_analysis'] = {
            'avg_cosine_similarity': float(avg_similarity.numpy()),
            'is_diverse': float(avg_similarity.numpy()) < 0.8  # é˜ˆå€¼å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        }

    # 6. æ£€æŸ¥æ¢¯åº¦ä¿¡æ¯
    test_input_var = tf.Variable(test_input)
    with tf.GradientTape() as tape:
        output = encoder(test_input_var, training=True)
        loss = tf.reduce_sum(output)

    # è®¡ç®—æ¢¯åº¦
    grads = tape.gradient(loss, encoder.trainable_variables)

    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
    has_gradients = any(grad is not None for grad in grads)
    gradient_norms = [tf.norm(grad).numpy() for grad in grads if grad is not None]

    results['gradient_analysis'] = {
        'has_gradients': has_gradients,
        'gradient_norms': gradient_norms,
        'total_gradient_norm': sum(gradient_norms) if has_gradients else 0
    }

    return results


def check_exploration(qmodel ):
    """
    æª¢æŸ¥æ¢ç´¢ç‹€æ…‹å’Œçµ±è¨ˆä¿¡æ¯

    Args:
        detailed: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯

    Returns:
        æ¢ç´¢ç‹€æ…‹çµ±è¨ˆå­—å…¸
    """
    total = qmodel.exploration_stats['total_choices']
    if total == 0:
        return {"status": "å°šæœªé€²è¡Œä»»ä½•é¸æ“‡"}

    exploration_rate = (qmodel.exploration_stats['exploration_choices'] / total) * 100
    exploitation_rate = (qmodel.exploration_stats['exploitation_choices'] / total) * 100

    stats = {
        'total_choices': total,
        'exploration_choices': qmodel.exploration_stats['exploration_choices'],
        'exploitation_choices': qmodel.exploration_stats['exploitation_choices'],
        'exploration_rate_percent': round(exploration_rate, 2),
        'exploitation_rate_percent': round(exploitation_rate, 2),
        'current_epsilon': round(qmodel.epsilon, 4),
        'epsilon_min': qmodel.epsilon_min,
        'last_choice_type': qmodel.exploration_stats['last_choice_type'],
        'is_exploring': exploration_rate > 20  # å¦‚æœæ¢ç´¢ç‡å¤§æ–¼20%ï¼Œèªç‚ºé‚„åœ¨æ¢ç´¢éšæ®µ
    }
    return stats

def check_training_process(qmodel, detailed=False, plot=False):
    """
    æª¢æŸ¥è¨“ç·´éç¨‹ç‹€æ…‹

    Args:
        detailed: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
        plot: æ˜¯å¦ç”Ÿæˆè¨“ç·´æ›²ç·šåœ–

    Returns:
        è¨“ç·´éç¨‹çµ±è¨ˆå­—å…¸
    """
    if not qmodel.training_history['episodes']:
        return {"status": "å°šæœªé–‹å§‹è¨“ç·´"}

    current_episode = qmodel.current_episode
    latest_reward = qmodel.training_history['rewards'][-1] if qmodel.training_history['rewards'] else 0
    avg_reward = qmodel.training_history['avg_rewards'][-1] if qmodel.training_history['avg_rewards'] else 0

    stats = {
        'current_episode': current_episode,
        'total_episodes': len(qmodel.training_history['episodes']),
        'latest_reward': round(latest_reward, 2),
        'latest_avg_reward': round(avg_reward, 2),
        'current_epsilon': round(qmodel.epsilon, 4),
        'q_table_size': len(qmodel.q_table),
        'training_duration': qmodel._get_training_duration(),
        'overall_avg_reward': round(np.mean(qmodel.training_history['rewards']), 2) if qmodel.training_history[
            'rewards'] else 0,'best_reward': round(max(qmodel.training_history['rewards']), 2) if qmodel.training_history['rewards'] else 0,
        'convergence_status': qmodel._check_convergence()
    }

    if detailed:
        # æ·»åŠ è©³ç´°çµ±è¨ˆ
        stats.update({
            'reward_trend': qmodel._get_reward_trend(),
            'exploration_trend': qmodel._get_exploration_trend(),
            'recent_performance': qmodel._get_recent_performance(10),
            'action_distribution': qmodel._get_current_action_distribution()
        })

    if plot:
        qmodel._plot_training_progress()

    return stats



def comprehensive_debug_checklist(qmodel, env, encoder):
    """
    å®Œæ•´çš„è°ƒè¯•æ£€æŸ¥æ¸…å•
    """
    print("=" * 60)
    print("ğŸ¤– å¼ºåŒ–å­¦ä¹ ç®—æ³•è°ƒè¯•æ£€æŸ¥æ¸…å•")
    print("=" * 60)

    results = {}

    # 1. æ£€æŸ¥å¥–åŠ±å‡½æ•°
    print("\n1. ğŸ¯ å¥–åŠ±å‡½æ•°æ£€æŸ¥:")
    results['reward_function'] = check_reward_function(qmodel)

    # 2. æ£€æŸ¥Qå€¼åˆç†æ€§
    print("\n2. ğŸ“Š Qå€¼åˆç†æ€§æ£€æŸ¥:")
    results['q_value_sanity'] = check_q_value_sanity(qmodel, encoder)

    # 3. æ£€æŸ¥æ¢ç´¢ç­–ç•¥
    print("\n3. ğŸ” æ¢ç´¢ç­–ç•¥æ£€æŸ¥:")
    results['exploration_check'] = check_exploration(qmodel)

    # 4. æ£€æŸ¥ç¥ç»ç½‘ç»œè®­ç»ƒ
    print("\n4. ğŸ§  ç¥ç»ç½‘ç»œè®­ç»ƒæ£€æŸ¥:")
    results['training_check'] = check_training_process(qmodel)

    # 5. æ£€æŸ¥çŠ¶æ€è¡¨ç¤º
    print("\n5. ğŸ“‹ çŠ¶æ€è¡¨ç¤ºæ£€æŸ¥:")
    results['state_representation'] = check_state_representation(encoder)

    return results


def check_reward_function(qmodel):
    """æ£€æŸ¥å¥–åŠ±å‡½æ•°"""
    test_cases = [
        # (label, bits, expected_reward)
        (0, [0, 0, 0, 0], "high"),  # æ‰€æœ‰å…³é—­åº”è¯¥é«˜å¥–åŠ±
        (1, [1, 1, 1, 1], "low"),  # æ‰€æœ‰å¼€å¯åº”è¯¥ä½å¥–åŠ±
        (0, [1, 0, 0, 0], "medium"),  # é€‚åº¦ä½¿ç”¨
    ]

    for label, bits, expectation in test_cases:
        reward = qmodel.compute_reward_batch(
            np.array([label]),
            np.array([bits])
        )[0]
        print(f"  æ ‡ç­¾ {label}, åŠ¨ä½œ {bits} â†’ å¥–åŠ±: {reward:.3f} ({expectation})")

    return "Reward function check completed"






def check_q_value_sanity(qmodel, encoder):
    """
    æ£€æŸ¥Qå€¼çš„åˆç†æ€§

    å‚æ•°:
        qmodel: Qå€¼æ¨¡å‹
        encoder: ç¼–ç å™¨æ¨¡å‹

    è¿”å›:
        dict: åŒ…å«Qå€¼æ£€æŸ¥ç»“æœçš„å­—å…¸
    """
    results = {}

    # 1. åˆ›å»ºæµ‹è¯•è¾“å…¥ï¼ˆæ³¨æ„åŒ¹é…ç¼–ç å™¨æœŸæœ›çš„è¾“å…¥å½¢çŠ¶ï¼‰
    batch_size, seq_len, input_dim = 1, 10, 7  # æ ¹æ®é”™è¯¯ä¿¡æ¯è°ƒæ•´

    # åˆ›å»ºç¬¦åˆç¼–ç å™¨è¾“å…¥å½¢çŠ¶çš„æµ‹è¯•æ•°æ®
    test_states = tf.random.normal((batch_size, seq_len, input_dim))

    # 2. è·å–çŠ¶æ€è¡¨ç¤º
    try:
        with tf.device('/CPU:0'):  # é¿å…GPUå†…å­˜é—®é¢˜
            #state_sequence = np.array(state_buffer).reshape(1, window_size, -1)
            #state_latent = encoder(state_sequence).numpy()
            state_representations = encoder(test_states, training=False)
        results['encoder_output_shape'] = state_representations.shape.as_list()
    except Exception as e:
        results['encoder_error'] = f"ç¼–ç å™¨å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}"
        return results

    # 3. æ£€æŸ¥Qå€¼æ¨¡å‹è¾“å…¥å…¼å®¹æ€§
    try:
        # åˆ›å»ºç¬¦åˆQå€¼æ¨¡å‹è¾“å…¥å½¢çŠ¶çš„æµ‹è¯•æ•°æ®
        if len(qmodel.inputs) > 0:
            q_input_shape = qmodel.inputs[0].shape.as_list()
            results['q_model_input_shape'] = q_input_shape

            # ç¡®ä¿çŠ¶æ€è¡¨ç¤ºä¸Qæ¨¡å‹è¾“å…¥å…¼å®¹
            if len(state_representations.shape) == len(q_input_shape):
                # è°ƒæ•´çŠ¶æ€è¡¨ç¤ºå½¢çŠ¶ä»¥åŒ¹é…Qæ¨¡å‹è¾“å…¥
                adjusted_states = state_representations
                if state_representations.shape[1:] != q_input_shape[1:]:
                    # å¦‚æœéœ€è¦é‡å¡‘
                    adjusted_states = tf.reshape(
                        state_representations,
                        [batch_size] + q_input_shape[1:]
                    )

                # è·å–Qå€¼
                q_values = qmodel(adjusted_states, training=False)
                results['q_values_shape'] = q_values.shape.as_list()

                # 4. åˆ†æQå€¼
                results['q_value_analysis'] = {
                    'mean': float(tf.reduce_mean(q_values).numpy()),
                    'std': float(tf.math.reduce_std(q_values).numpy()),
                    'min': float(tf.reduce_min(q_values).numpy()),
                    'max': float(tf.reduce_max(q_values).numpy()),
                    'range': float(tf.reduce_max(q_values).numpy() - tf.reduce_min(q_values).numpy()),
                    'has_nan': tf.reduce_any(tf.math.is_nan(q_values)).numpy(),
                    'has_inf': tf.reduce_any(tf.math.is_inf(q_values)).numpy(),
                }

                # 5. æ£€æŸ¥Qå€¼æ˜¯å¦è¿‡äºæç«¯
                q_values_flat = tf.reshape(q_values, [-1])
                extreme_values = tf.reduce_sum(
                    tf.cast(tf.abs(q_values_flat) > 100.0, tf.float32)
                ) / tf.cast(tf.size(q_values_flat), tf.float32)

                results['extreme_value_analysis'] = {
                    'extreme_value_ratio': float(extreme_values.numpy()),
                    'has_extreme_values': float(extreme_values.numpy()) > 0.1
                }

            else:
                results[
                    'shape_mismatch'] = f"å½¢çŠ¶ä¸åŒ¹é…: ç¼–ç å™¨è¾“å‡º {state_representations.shape}, Qæ¨¡å‹è¾“å…¥ {q_input_shape}"
        else:
            results['q_model_error'] = "æ— æ³•è·å–Qæ¨¡å‹çš„è¾“å…¥å½¢çŠ¶"

    except Exception as e:
        results['q_model_error'] = f"Qæ¨¡å‹å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}"

    # 6. æ£€æŸ¥æ¢¯åº¦è®¡ç®—
    try:
        test_states_var = tf.Variable(test_states)
        with tf.GradientTape() as tape:
            state_repr = encoder(test_states_var, training=True)
            if len(state_repr.shape) == len(qmodel.inputs[0].shape):
                adjusted_states = tf.reshape(
                    state_repr,
                    [batch_size] + qmodel.inputs[0].shape[1:].as_list()
                )
                q_vals = qmodel(adjusted_states, training=True)
                loss = tf.reduce_mean(q_vals)

                grads = tape.gradient(loss, qmodel.trainable_variables)
                has_gradients = any(grad is not None for grad in grads)

                results['gradient_check'] = {
                    'has_gradients': has_gradients,
                    'gradient_norms': [float(tf.norm(grad).numpy()) for grad in grads if grad is not None]
                }
    except Exception as e:
        results['gradient_error'] = f"æ¢¯åº¦è®¡ç®—å¤±è´¥: {str(e)}"

    return results

import csv

class TrainingMonitor:
    def __init__(self, save_path=None):
        self.episodes = []
        self.rewards = []
        self.avg_qs = []
        self.save_path = save_path

    def record_episode(self, episode, reward, avg_q):
        """è®°å½•ä¸€æ¬¡è®­ç»ƒçš„ç»“æœ"""
        self.episodes.append(episode)
        self.rewards.append(reward)
        self.avg_qs.append(avg_q)

        print(f"[Monitor] Episode {episode} | Reward={reward:.2f} | AvgQ={avg_q:.2f}")

    def save_to_csv(self, filename=None):
        """ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶"""
        if filename is None:
            if self.save_path:
                filename = self.save_path
            else:
                filename = "training_log.csv"

        with open(filename, "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["Episode", "Reward", "AvgQ"])
            for e, r, q in zip(self.episodes, self.rewards, self.avg_qs):
                writer.writerow([e, r, q])
        print(f"[Monitor] Results saved to {filename}")

    def plot(self, show=True, save_path=None):
        """ç»˜åˆ¶ reward å’Œ avg_q æ›²çº¿"""
        plt.figure(figsize=(10, 5))

        plt.subplot(1, 2, 1)
        plt.plot(self.episodes, self.rewards, label="Reward")
        plt.xlabel("Episode")
        plt.ylabel("Reward")
        plt.title("Reward per Episode")
        plt.grid(True)
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(self.episodes, self.avg_qs, label="Avg Q-value", color="orange")
        plt.xlabel("Episode")
        plt.ylabel("Avg Q")
        plt.title("Average Q per Episode")
        plt.grid(True)
        plt.legend()

        plt.tight_layout()
        if save_path:
            plt.savefig(save_path)
            print(f"[Monitor] Plot saved to {save_path}")
        if show:
            plt.show()

def diagnostic_training(model, encoder, env, monitor, episodes=50):
    """è¯Šæ–­è®­ç»ƒè¿‡ç¨‹ - ä¿®å¾©NoneTypeéŒ¯èª¤ç‰ˆæœ¬"""
    print("é–‹å§‹è¨ºæ–·è¨“ç·´...")

    for episode in range(episodes):
        try:
            # ç’°å¢ƒé‡ç½®
            reset_result = env.reset()
            print(f"é‡ç½®çµæœé¡å‹: {type(reset_result)}")

            # è§£æé‡ç½®çµæœ
            state, info = _parse_env_result(reset_result)
            print(f"è§£æå¾Œç‹€æ…‹é¡å‹: {type(state)}, å€¼: {state}")

            # æª¢æŸ¥ä¸¦æ¸…ç†ç‹€æ…‹æ•¸æ“š
            state = _clean_and_validate_state(state)
            print(f"æ¸…ç†å¾Œç‹€æ…‹: {state}")

            total_reward = 0
            episode_q_values = []
            done = False
            step = 0

            while not done and step < 100:
                try:
                    # æª¢æŸ¥ç‹€æ…‹æœ‰æ•ˆæ€§
                    if state is None:
                        print("ç‹€æ…‹ç‚ºNoneï¼Œè·³éæ­¤æ­¥é©Ÿ")
                        break

                    # é‡å¡‘ç‹€æ…‹ä¸¦æª¢æŸ¥å½¢ç‹€
                    state_reshaped = _safe_reshape(state)
                    print(f"é‡å¡‘å¾Œç‹€æ…‹å½¢ç‹€: {state_reshaped.shape}")

                    # ä½¿ç”¨ç·¨ç¢¼å™¨ç²å–æ½›åœ¨è¡¨ç¤º
                    state_latent = encoder(state_reshaped).numpy()
                    print(f"æ½›åœ¨ç‹€æ…‹å½¢ç‹€: {state_latent.shape}")

                    # é¸æ“‡å‹•ä½œ
                    action = model.choose_action(state_latent)
                    print(f"é¸æ“‡å‹•ä½œ: {action}")

                    # åŸ·è¡Œå‹•ä½œ
                    step_result = env.step(action)
                    next_state, reward, done, info = _parse_env_result(step_result)

                    # æ¸…ç†ä¸‹ä¸€å€‹ç‹€æ…‹
                    next_state = _clean_and_validate_state(next_state)
                    print(f"ä¸‹ä¸€æ­¥ç‹€æ…‹: {next_state}")

                    # ç²å–ä¸‹ä¸€å€‹æ½›åœ¨ç‹€æ…‹
                    next_state_reshaped = _safe_reshape(next_state)
                    next_state_latent = encoder(next_state_reshaped).numpy()

                    # é¸æ“‡ä¸‹ä¸€å€‹å‹•ä½œ
                    next_action = model.choose_action(next_state_latent)

                    # æ›´æ–°æ¨¡å‹
                    model.update(state_latent, action, reward, next_state_latent, next_action)

                    # è¨˜éŒ„æ•¸æ“š
                    total_reward += reward
                    q_values = model.get_q_values(state_latent)
                    episode_q_values.append(q_values)

                    # æ›´æ–°ç‹€æ…‹
                    state = next_state
                    step += 1

                    print(f"æ­¥é©Ÿ {step}: çå‹µ={reward}, ç¸½çå‹µ={total_reward}")

                    # ç›£æ§
                    if monitor and step % 10 == 0:
                        monitor.record_step(episode, step, reward, q_values, action)

                except Exception as e:
                    print(f"æ­¥é©Ÿ {step} å‡ºéŒ¯: {e}")
                    import traceback
                    traceback.print_exc()
                    break

            # è¨˜éŒ„å›åˆ
            if monitor:
                avg_q = np.mean(episode_q_values) if episode_q_values else 0
                monitor.record_episode(episode, total_reward, avg_q)

            # è¨ºæ–·ä¿¡æ¯
            print(f"Episode {episode}: ç¸½çå‹µ={total_reward}, æ­¥æ•¸={step}")
            _print_diagnostic_info(model, episode, total_reward, step)

        except Exception as e:
            print(f"å›åˆ {episode} åˆå§‹åŒ–å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
            continue


def _clean_and_validate_state(state):
    """æ¸…ç†å’Œé©—è­‰ç‹€æ…‹æ•¸æ“š"""
    if state is None:
        print("è­¦å‘Š: ç‹€æ…‹ç‚ºNoneï¼Œä½¿ç”¨é»˜èªç‹€æ…‹")
        return np.zeros(1)  # è¿”å›é»˜èªç‹€æ…‹

    # å¦‚æœç‹€æ…‹æ˜¯å­—å…¸ï¼Œå˜—è©¦æå–æ•¸å€¼
    if isinstance(state, dict):
        print("ç‹€æ…‹æ˜¯å­—å…¸ï¼Œå˜—è©¦æå–æ•¸å€¼...")
        # å˜—è©¦æ‰¾åˆ°åŒ…å«æ•¸å€¼çš„éµ
        for key, value in state.items():
            if value is not None and isinstance(value, (int, float, np.ndarray, list)):
                print(f"ä½¿ç”¨éµ '{key}' çš„å€¼: {value}")
                state = value
                break
        else:
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°åˆé©çš„å€¼ï¼Œä½¿ç”¨æ‰€æœ‰å€¼
            values = list(state.values())
            if all(v is not None for v in values):
                state = values
            else:
                print("å­—å…¸ä¸­åŒ…å«Noneå€¼ï¼Œä½¿ç”¨é»˜èªç‹€æ…‹")
                return np.zeros(1)

    # è½‰æ›ç‚ºnumpyæ•¸çµ„
    if not isinstance(state, np.ndarray):
        try:
            state = np.array(state, dtype=np.float32)
        except Exception as e:
            print(f"è½‰æ›ç‚ºæ•¸çµ„å¤±æ•—: {e}, ä½¿ç”¨é»˜èªç‹€æ…‹")
            return np.zeros(1)

    # æª¢æŸ¥æ˜¯å¦åŒ…å«Noneå€¼
    if np.any(state == None):  # noqa: E711
        print("ç‹€æ…‹ä¸­åŒ…å«Noneå€¼ï¼Œé€²è¡Œæ¸…ç†...")
        # å°‡Noneæ›¿æ›ç‚º0
        state = np.where(state == None, 0, state)  # noqa: E711

    # æª¢æŸ¥NaNå€¼
    if np.any(np.isnan(state)):
        print("ç‹€æ…‹ä¸­åŒ…å«NaNå€¼ï¼Œé€²è¡Œæ¸…ç†...")
        state = np.nan_to_num(state)

    return state


def _safe_reshape(state):
    """å®‰å…¨åœ°é‡å¡‘ç‹€æ…‹"""
    # ç¢ºä¿æ˜¯numpyæ•¸çµ„
    if not isinstance(state, np.ndarray):
        state = np.array(state)

    # æª¢æŸ¥ç¶­åº¦
    if state.ndim == 1:
        return state.reshape(1, -1)
    elif state.ndim == 2:
        return state
    else:
        print(f"ä¸æ”¯æŒçš„ç‹€æ…‹ç¶­åº¦: {state.ndim}, å˜—è©¦å±•å¹³")
        return state.flatten().reshape(1, -1)


def _parse_env_result(env_result):
    """è§£æç’°å¢ƒè¿”å›çµæœ - åŠ å¼·ç‰ˆæœ¬"""
    print(f"è§£æç’°å¢ƒçµæœ: {type(env_result)}")

    if env_result is None:
        print("ç’°å¢ƒè¿”å›Noneï¼Œä½¿ç”¨é»˜èªå€¼")
        return np.zeros(1), {}

    if isinstance(env_result, tuple):
        print(f"å…ƒçµ„é•·åº¦: {len(env_result)}")
        if len(env_result) == 2:
            state, info = env_result
            return state, info if isinstance(info, dict) else {}
        elif len(env_result) == 4:
            state, reward, done, info = env_result
            return state, info
        else:
            print(f"æœªçŸ¥çš„å…ƒçµ„é•·åº¦: {len(env_result)}")
            return env_result[0], {}

    elif isinstance(env_result, dict):
        print(f"å­—å…¸éµ: {list(env_result.keys())}")
        state = env_result.get('observation',
                               env_result.get('state',
                                              env_result.get('obs',
                                                             None)))

        if state is None:
            # å˜—è©¦æ‰¾åˆ°ç¬¬ä¸€å€‹åˆé©çš„å€¼
            for key, value in env_result.items():
                if value is not None and not isinstance(value, (str, dict, list)):
                    state = value
                    break
            else:
                state = np.zeros(1)

        info = {k: v for k, v in env_result.items()
                if k not in ['observation', 'state', 'obs', 'reward', 'done']}
        return state, info

    else:
        print(f"å–®ä¸€è¿”å›å€¼: {env_result}")
        return env_result, {}


def _print_diagnostic_info(model, episode, reward, steps):
    """æ‰“å°è¨ºæ–·ä¿¡æ¯"""
    try:
        stats = model.check_training_process(detailed=False)
        print(f"  æ¢ç´¢ç‡: {stats.get('exploration_rate_percent', 0):.1f}%")
        print(f"  Q-tableå¤§å°: {stats.get('q_table_size', 0)}")
    except Exception as e:
        print(f"ç²å–è¨ºæ–·ä¿¡æ¯å¤±æ•—: {e}")



def _ensure_array_format(data):
    """ç¢ºä¿æ•¸æ“šæ˜¯numpyæ•¸çµ„æ ¼å¼"""
    if isinstance(data, np.ndarray):
        return data
    elif isinstance(data, (list, tuple)):
        return np.array(data)
    elif isinstance(data, dict):
        # å˜—è©¦å¾å­—å…¸ä¸­æå–æ•¸å€¼æ•¸æ“š
        numeric_values = [v for v in data.values() if isinstance(v, (int, float))]
        if numeric_values:
            return np.array(numeric_values)
        else:
            return np.array(list(data.values()))
    else:
        return np.array([data])




class RL_Debugger:
    def __init__(self):
        self.history = {
            'rewards': [], 'q_values': [], 'actions': [],
            'losses': [], 'epsilons': [], 'correlations': []
        }

    def log_episode(self, rewards, q_values, actions, loss, epsilon):
        self.history['rewards'].append(np.mean(rewards))
        self.history['q_values'].append(np.mean(q_values))
        self.history['actions'].extend(actions)
        self.history['losses'].append(loss)
        self.history['epsilons'].append(epsilon)

        # è®¡ç®—ç›¸å…³æ€§
        if len(rewards) > 1 and len(q_values) > 1:
            corr = np.corrcoef(rewards, q_values)[0, 1]
            self.history['correlations'].append(corr)

    def plot_diagnostics(self):
        fig, axes = plt.subplots(3, 2, figsize=(15, 12))

        # 1. å¥–åŠ±è¶‹åŠ¿
        axes[0, 0].plot(self.history['rewards'])
        axes[0, 0].set_title('Average Reward per Episode')
        axes[0, 0].set_ylabel('Reward')

        # 2. Qå€¼è¶‹åŠ¿
        axes[0, 1].plot(self.history['q_values'])
        axes[0, 1].set_title('Average Q-value per Episode')
        axes[0, 1].set_ylabel('Q-value')

        # 3. æŸå¤±å‡½æ•°
        axes[1, 0].plot(self.history['losses'])
        axes[1, 0].set_title('Training Loss')
        axes[1, 0].set_ylabel('Loss')
        axes[1, 0].set_yscale('log')  # å¯¹æ•°å°ºåº¦çœ‹å˜åŒ–

        # 4. æ¢ç´¢ç‡
        axes[1, 1].plot(self.history['epsilons'])
        axes[1, 1].set_title('Exploration Rate (Epsilon)')
        axes[1, 1].set_ylabel('Epsilon')

        # 5. åŠ¨ä½œåˆ†å¸ƒ
        action_counts = pd.Series(self.history['actions']).value_counts()
        axes[2, 0].bar(action_counts.index, action_counts.values)
        axes[2, 0].set_title('Action Distribution')
        axes[2, 0].set_xlabel('Action')
        axes[2, 0].set_ylabel('Count')

        # 6. ç›¸å…³æ€§è¶‹åŠ¿
        axes[2, 1].plot(self.history['correlations'])
        axes[2, 1].set_title('Reward-Q Correlation')
        axes[2, 1].set_ylabel('Correlation')
        axes[2, 1].axhline(y=0, color='r', linestyle='--')

        plt.tight_layout()
        plt.show()


def run_comprehensive_debug(qmodel, encoder, env, data_files):
    """
    æ‰§è¡Œå®Œæ•´çš„è°ƒè¯•æµç¨‹
    """
    print("å¼€å§‹å…¨é¢è°ƒè¯•...")

    # 1. è¿è¡ŒåŸºæœ¬æ£€æŸ¥
    checklist_results = comprehensive_debug_checklist(qmodel, env, encoder)

    # 2. æ”¶é›†è®­ç»ƒæ•°æ®
    monitor = create_training_monitor()

    # 3. è¿›è¡ŒçŸ­æœŸè®­ç»ƒå¹¶ç›‘æ§
    print("\nè¿›è¡Œè¯Šæ–­è®­ç»ƒ...")
    monitor = TrainingMonitor(save_path="training_results.csv")

    diagnostic_training(qmodel, encoder, env, monitor, episodes=50)

    # 4. åˆ†æç»“æœ
    #print("\nåˆ†æè°ƒè¯•ç»“æœ...")
    #analyze_debug_results(monitor, checklist_results)

    #5. æä¾›è°ƒè¯•å»ºè®®
    #provide_debug_recommendations(monitor)


# ä½¿ç”¨ç¤ºä¾‹
#if __name__ == "__main__":
# åˆå§‹åŒ–ä½ çš„ç»„ä»¶
# qmodel = YourDQNAgent(...)
# encoder = YourEncoder(...)
# env = YourEnvironment(...)

# è¿è¡Œè°ƒè¯•
# run_comprehensive_debug(qmodel, encoder, env, files)

# æˆ–è€…å•ç‹¬è¿è¡ŒæŸäº›è°ƒè¯•åŠŸèƒ½
#debugger = RL_Debugger()
#hyperparameter_tuning(qmodel, encoder, env)

