# server_side_processor_tf.py
import tensorflow as tf
from tensorflow import keras
import numpy as np
import zlib
import sys
import json
import os
import base64
from datetime import datetime
import tensorflow_model_optimization as tfmot
from typing import Dict, Any, Optional, Union

import tensorflow_probability as tfp
from tensorflow.python.ops.metrics_impl import false_negatives

tfd = tfp.distributions

from util_exporter import TensorFlowESP32Exporter
from util_agent import ESP32OnlinePPOFisherAgent
from util_env import PlantLLLHVACEnv
from util_trainer import LLLTrainer

MODEL_DIR = "./models"

def ewc_update(actor, fisher, optimal_params, current_params, learning_rate=1e-3, ewc_lambda=500):
    """
    ä½¿ç”¨ Fisher çŸ©é™£æ›´æ–° Actor åƒæ•¸
    actor: tf.keras.Sequential
    fisher: dict
    optimal_params: dict
    current_params: dict, å¯ç”¨ actor.trainable_variables
    """
    with tf.GradientTape() as tape:
        loss = 0.0
        for var in actor.trainable_variables:
            name = var.name
            diff = var - optimal_params[name]
            loss += tf.reduce_sum(fisher[name] * diff ** 2)
        loss *= ewc_lambda

    grads = tape.gradient(loss, actor.trainable_variables)
    for var, g in zip(actor.trainable_variables, grads):
        if g is not None:
            var.assign_sub(learning_rate * g)




def generate_smart_representative_data(env, num_samples=1000, mode_weights=None, return_labels=False):
    """
    æ™ºèƒ½ç”Ÿæˆä»£è¡¨æ€§æ•°æ®ï¼Œè¦†ç›–ä¸åŒæ¨¡å¼å’Œå·¥å†µ

    Args:
        env: PlantLLLHVACEnvå®ä¾‹
        num_samples: æ€»æ ·æœ¬æ•°
        mode_weights: ä¸åŒæ¨¡å¼çš„æƒé‡ [growing, flowering, seeding]
        return_labels: æ˜¯å¦åŒæ—¶è¿”å›æ ‡ç­¾ (X, y)
    """
    if mode_weights is None:
        mode_weights = [0.4, 0.3, 0.3]  # é»˜è®¤æƒé‡

    all_data = []
    all_labels = []

    # ä¸ºæ¯ä¸ªæ¨¡å¼ç”Ÿæˆæ•°æ®
    modes = ["growing", "flowering", "seeding"]

    for mode, weight in zip(modes, mode_weights):
        num_mode_samples = int(num_samples * weight)
        print(f"ä¸ºæ¨¡å¼ '{mode}' ç”Ÿæˆ {num_mode_samples} ä¸ªæ ·æœ¬")

        # è®¾ç½®ç¯å¢ƒæ¨¡å¼
        env.mode = mode
        env.reset()

        action = [1, 0, 0, 0, 0, 1, 0, 0]
        # ç”Ÿæˆè¯¥æ¨¡å¼çš„æ•°æ®
        for i in range(num_mode_samples):


            # æ‰§è¡ŒåŠ¨ä½œ
            true_label = modes.index(mode)
            next_state, reward, done, info = env.step(action , true_label=true_label)

            # æ·»åŠ æ•°æ®ç‚¹ & æ ‡ç­¾
            current_data_point = env.current_sequence[0, -1]
            all_data.append(current_data_point)
            all_labels.append(true_label)

            if done:
                env.reset()
            print("New state:", next_state)
            print("Reward:",  reward)
    # è½¬ numpy
    all_data = np.array(all_data, dtype=np.float32)
    all_labels = np.array(all_labels, dtype=np.int32)

    # éšæœºæ‰“ä¹±
    idx = np.arange(len(all_data))
    np.random.shuffle(idx)
    all_data, all_labels = all_data[idx], all_labels[idx]

    if return_labels:
        return all_data[:num_samples], all_labels[:num_samples]
    else:
        return all_data[:num_samples]

def env_pipe_trainer(lll_model=None,num_tasks=3,num_classes=3,num_epochs_per_task=3,batch_size=32, learning_rate=0.001, ewc_lambda=0.4):
    #env_lll_model, state_dim, action_dim, hidden_units,learning_rate=0.001, ewc_lambda=0.4

    trainer = LLLTrainer(lll_model=lll_model, learning_rate=0.001, ewc_lambda=0.4)
    latent_dim=trainer.latent_dim
    # æ¨¡æ‹Ÿå¤šä»»åŠ¡æ•°æ®
    for task_id in range(num_tasks):
        print(f"\n=== Training Task {task_id + 1} ===")
        num_samples = 200
        latent_features = np.random.randn(num_samples, latent_dim).astype(np.float32)
        labels = np.random.randint(0, num_classes, size=(num_samples,)).astype(np.int32)
        loss=0
        # æŒ‰ batch è®­ç»ƒå½“å‰ä»»åŠ¡
        for epoch in range(num_epochs_per_task):
            indices = np.random.permutation(num_samples)
            latent_features_shuffled = latent_features[indices]
            labels_shuffled = labels[indices]

            for start_idx in range(0, num_samples, batch_size):
                end_idx = start_idx + batch_size
                batch_latent = latent_features_shuffled[start_idx:end_idx]
                batch_labels = labels_shuffled[start_idx:end_idx]
                loss = trainer._train_lll_model(batch_latent, batch_labels)
            print(f"  Epoch {epoch + 1}, Last batch loss: {loss:.4f}")

        # è®­ç»ƒå®Œå½“å‰ä»»åŠ¡åï¼Œæ›´æ–° EWC ä¿¡æ¯
        trainer.update_ewc(latent_features, labels)


def verify_tflite_model(file_path):
    """æ­£ç¡®çš„TFLiteæ¨¡å‹éªŒè¯å‡½æ•°"""
    try:
        with open(file_path, 'rb') as f:
            model_data = f.read()

        if len(model_data) < 16:
            print(" æ–‡ä»¶å¤ªå°")
            return False

        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {len(model_data)} å­—èŠ‚")
        print(f"ğŸ“Š å‰16å­—èŠ‚: {model_data[:16].hex(' ')}")

        # æ­£ç¡®çš„TFLiteé­”æœ¯æ•°å­—æ£€æµ‹ï¼ˆä»ç¬¬5å­—èŠ‚å¼€å§‹ï¼‰
        if len(model_data) >= 8:
            if model_data[4:8] == b'TFL3':
                print(" æ£€æµ‹åˆ°æœ‰æ•ˆçš„TFLiteé­”æœ¯æ•°å­— (TFL3)")
                magic_ok = True
            else:
                print(f" æ— æ•ˆçš„TFLiteé­”æœ¯æ•°å­—: {model_data[4:8].hex(' ')}")
                magic_ok = False
        else:
            magic_ok = False

        # ç”¨TensorFlowéªŒè¯ï¼ˆè¿™æ˜¯æœ€å¯é çš„ï¼‰
        try:
            interpreter = tf.lite.Interpreter(model_content=model_data)
            interpreter.allocate_tensors()
            print(" TensorFlowéªŒè¯æˆåŠŸ")
            tf_ok = True
        except Exception as e:
            print(f" TensorFlowéªŒè¯å¤±è´¥: {e}")
            tf_ok = False

        # æœ€ç»ˆç»“æœï¼šåªè¦TensorFlowéªŒè¯æˆåŠŸå°±è®¤ä¸ºæœ‰æ•ˆ
        if tf_ok:
            if not magic_ok:
                print("ï¸  æ³¨æ„ï¼šæ–‡ä»¶ç»“æ„ç‰¹æ®Šï¼Œä½†TensorFlowå¯ä»¥åŠ è½½")
            return True
        else:
            return False

    except Exception as e:
        print(f" æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return False

def debug_model_creation(representative_data):
    print("ğŸ” å¼€å§‹è¯¦ç»†è°ƒè¯•æ¨¡å‹åˆ›å»ºè¿‡ç¨‹...")

    # 1. åˆ›å»ºOTAåŒ…
    ota_package = exporter.create_ota_package(representative_data, quantize=True)
    print(f"âœ… OTAåŒ…åˆ›å»ºæˆåŠŸ")
    print(f"OTAåŒ…é”®å€¼: {list(ota_package.keys())}")

    # 2. æ£€æŸ¥model_data_b64
    model_data_b64 = ota_package['model_data_b64']
    print(f"Base64æ•°æ®é•¿åº¦: {len(model_data_b64)} å­—ç¬¦")
    print(f"Base64å‰50å­—ç¬¦: {repr(model_data_b64[:50])}")

    # 3. Base64è§£ç 
    try:
        raw_bytes = base64.b64decode(model_data_b64)
        print(f"âœ… Base64è§£ç æˆåŠŸ")
        print(f"è§£ç åå¤§å°: {len(raw_bytes)} å­—èŠ‚")
        print(f"å‰32å­—èŠ‚: {raw_bytes[:32].hex(' ')}")
    except Exception as e:
        print(f"âŒ Base64è§£ç å¤±è´¥: {e}")
        return False

    # 4. åˆ†ææ•°æ®æ ¼å¼
    print("\nğŸ” æ•°æ®åˆ†æ:")

    # æ£€æŸ¥æ˜¯å¦æ˜¯TFLiteæ ¼å¼ - ä¿®æ­£åçš„é€»è¾‘
    if len(raw_bytes) >= 8:  # éœ€è¦è‡³å°‘8å­—èŠ‚æ‰èƒ½æ£€æŸ¥çœŸæ­£çš„é­”æœ¯æ•°å­—
        # çœŸæ­£çš„TFLiteé­”æœ¯æ•°å­—åœ¨ç¬¬5-8å­—èŠ‚ï¼ˆç´¢å¼•4-7ï¼‰
        magic = raw_bytes[4:8]
        if magic == b'TFL3':
            print("âœ… æ£€æµ‹åˆ°æœ‰æ•ˆçš„TFLiteé­”æœ¯æ•°å­— (TFL3)")
            print(f"é­”æœ¯æ•°å­—ä½ç½®: å­—èŠ‚4-7: {magic.hex(' ')}")
        else:
            print(f"âŒ ä¸æ˜¯TFLiteæ ¼å¼: å­—èŠ‚4-7 = {magic.hex(' ')}")
            print(f"æœŸæœ›: 54 46 4C 33 (TFL3)")

        # æ˜¾ç¤ºå®Œæ•´çš„å‰16å­—èŠ‚ç”¨äºè°ƒè¯•
        print(f"å‰16å­—èŠ‚å®Œæ•´æ•°æ®: {raw_bytes[:16].hex(' ')}")
        print(f"å­—èŠ‚0-3: {raw_bytes[:4].hex(' ')} (FlatBufferå¤´)")
        print(f"å­—èŠ‚4-7: {raw_bytes[4:8].hex(' ')} (TFLiteé­”æœ¯æ•°å­—)")
        print(f"å­—èŠ‚8-15: {raw_bytes[8:16].hex(' ')} (å…¶ä»–å…ƒæ•°æ®)")
    else:
        print("âŒ æ•°æ®å¤ªçŸ­ï¼Œæ— æ³•æ£€æŸ¥TFLiteé­”æœ¯æ•°å­—")

    # æ£€æŸ¥æ˜¯å¦æ˜¯å…¶ä»–æ ¼å¼
    common_formats = {
        b'\x78\x9C': 'Zlibå‹ç¼©',
        b'\x1F\x8B': 'Gzipå‹ç¼©',
        b'PK': 'Zipå‹ç¼©',
        b'{': 'JSONæ ¼å¼',
        b'<': 'XMLæ ¼å¼',
    }

    for magic, format_name in common_formats.items():
        if raw_bytes.startswith(magic):
            print(f"âš ï¸  æ£€æµ‹åˆ°å¯èƒ½æ˜¯ {format_name}")

    # 5. å°è¯•ç›´æ¥éªŒè¯
    try:
        interpreter = tf.lite.Interpreter(model_content=raw_bytes)
        interpreter.allocate_tensors()
        print("âœ… ç›´æ¥éªŒè¯æˆåŠŸ - æ•°æ®å·²ç»æ˜¯TFLiteæ ¼å¼")

        # ä¿å­˜æ–‡ä»¶
        path = os.path.join(MODEL_DIR, "esp32_optimized_model.tflite")
        with open(path, 'wb') as f:
            f.write(raw_bytes)
        print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {path}")
        return True

    except Exception as e:
        print(f"âŒ ç›´æ¥éªŒè¯å¤±è´¥: {e}")

    # 6. å°è¯•ä½œä¸ºæ–‡æœ¬è§£ç ï¼ˆå¯èƒ½æ˜¯é”™è¯¯ä¿¡æ¯ï¼‰
    try:
        text_content = raw_bytes.decode('utf-8')
        print(f"âš ï¸  æ•°æ®å¯ä»¥è§£ç ä¸ºæ–‡æœ¬:")
        print(f"æ–‡æœ¬å†…å®¹: {text_content[:200]}...")

        # å¦‚æœæ˜¯JSONï¼Œå°è¯•è§£æ
        if text_content.strip().startswith('{'):
            import json
            try:
                json_data = json.loads(text_content)
                print(f"âœ… æ˜¯JSONæ ¼å¼: {list(json_data.keys())}")
            except:
                print("âŒ ä¸æ˜¯æœ‰æ•ˆçš„JSON")

    except UnicodeDecodeError:
        print("âš ï¸  æ•°æ®ä¸æ˜¯æ–‡æœ¬æ ¼å¼")

    return False


def analyze_model_details(model_bytes):
    """è¯¦ç»†åˆ†ææ¨¡å‹å†…å®¹"""
    interpreter0 = tf.lite.Interpreter(model_content=model_bytes)
    interpreter0.allocate_tensors()

    print("=== æ¨¡å‹è¯¦ç»†ä¿¡æ¯ ===")
    print(f"TensorFlow Liteç‰ˆæœ¬: {tf.__version__}")

    # è·å–æ‰€æœ‰æ“ä½œç¬¦
    print("\n=== æ“ä½œç¬¦åˆ—è¡¨ ===")
    for i, op in enumerate(interpreter0._get_ops_details()):
        print(f"{i}: {op['op_name']} (index: {op['index']})")

    # è¾“å…¥è¾“å‡ºè¯¦æƒ…
    print("\n=== è¾“å…¥å¼ é‡ ===")
    for i, detail in enumerate(interpreter0.get_input_details()):
        print(f"Input {i}: {detail['name']} {detail['shape']} {detail['dtype']}")

    print("\n=== è¾“å‡ºå¼ é‡ ===")
    for i, detail in enumerate(interpreter0.get_output_details()):
        print(f"Output {i}: {detail['name']} {detail['shape']} {detail['dtype']}")

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸æ”¯æŒçš„æ“ä½œç¬¦
    micro_supported_ops = ['FULLY_CONNECTED', 'SOFTMAX', 'RESHAPE', 'QUANTIZE', 'DEQUANTIZE']
    all_ops = [op['op_name'] for op in interpreter0._get_ops_details()]

    print("\n=== å…¼å®¹æ€§æ£€æŸ¥ ===")
    for op in all_ops:
        if op not in micro_supported_ops:
            print(f"ï¸  å¯èƒ½ä¸æ”¯æŒçš„æ“ä½œç¬¦: {op}")
        else:
            print(f" æ”¯æŒçš„æ“ä½œç¬¦: {op}")



# ä½¿ç”¨ç¤ºä¾‹
def trainbytask_lifelong_ppo(env,agent,tasks):


   
    agent.reset_ewc_variables()
    # æ”¶é›†ç»éªŒ
    experiences = agent.collect_experiences(agent, env, num_episodes=10)

    # åˆ†ææ”¶é›†åˆ°çš„ç»éªŒ
    print(f"æ€»å…±æ”¶é›†äº† {len(experiences)} æ¡ç»éªŒ")
    states_batch, actions_batch, advantages_batch, old_probs_batch, returns_batch = agent.process_experiences(agent,experiences)

    # è®­ç»ƒå¤šä¸ªepoch
    for epoch in range(100):
        # è®­ç»ƒæ­¥éª¤
        # ä¿®æ­£ï¼šç¡®ä¿åŠ¨ä½œå¼ é‡ä¿æŒä¸ºint32
        states_tensor = tf.convert_to_tensor(states_batch, dtype=tf.float32)
        actions_tensor = tf.convert_to_tensor(actions_batch, dtype=tf.int32)  # ä¿æŒint32
        old_probs_tensor = tf.convert_to_tensor(old_probs_batch, dtype=tf.float32)
        returns_tensor = tf.convert_to_tensor(returns_batch, dtype=tf.float32)
        advantages_tensor = tf.convert_to_tensor(returns_batch, dtype=tf.float32)
        try:
            
            total_loss, policy_loss, value_loss, entropy, ewc_loss = agent.train_step_onehot(
                states=states_tensor,
                actions=actions_tensor,
                advantages=advantages_tensor,
                old_probs=old_probs_tensor,
                returns=returns_tensor,
                use_ewc=False
            )
        except Exception as e:
            print(f"è®­ç»ƒé”™è¯¯: {e}")
            print(f"actions_tensor dtype: {actions_tensor.dtype}")
            print(f"actions_tensor shape: {actions_tensor.shape}")
            raise
        print(f" epoch {epoch} loss={total_loss:.4f}")
    # ä¿å­˜å½“å‰ä»»åŠ¡çŸ¥è¯†
    agent.save_task_knowledge((states_batch, actions_batch, advantages_batch, old_probs_batch, returns_batch))

    # æµ‹è¯•æ‰€æœ‰å·²å­¦ä»»åŠ¡çš„æ€§èƒ½ï¼ˆæ£€æŸ¥æ˜¯å¦é—å¿˜ï¼‰
    #for test_id in range(task_id + 1):
    performance = agent.test_task_performance(tasks)
    print(f"æµ‹è¯•æ€§èƒ½: {performance}")

    # å›æ”¾ä¹‹å‰ä»»åŠ¡çš„ç»éªŒ
    for _ in range(5):
        agent.replay_previous_tasks(batch_size=32)
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    rewards = [exp['reward'] for exp in experiences]
    health_statuses = [exp['info']['health_status'] for exp in experiences]

    print(f"å¹³å‡å¥–åŠ±: {np.mean(rewards):.3f}")
    print(f"å¥åº·æ¯”ä¾‹: {np.mean([1 if s == 0 else 0 for s in health_statuses]) * 100:.1f}%")
    print(f"ä¸å¥åº·æ¯”ä¾‹: {np.mean([1 if s == 1 else 0 for s in health_statuses]) * 100:.1f}%")
 

# -----------------------------
# ä½¿ç”¨ç¯„ä¾‹ï¼ˆä¿®æ­£æˆå¯ load çš„å®Œæ•´æ¨¡å‹æª”ï¼‰
# -----------------------------
if __name__ == "__main__":
    # ä»£è¡¨æ€§è³‡æ–™ï¼ˆè«‹æ›¿æ›æˆä½ çš„å¯¦éš›è³‡æ–™ï¼‰
    # åˆ›å»ºç¯å¢ƒ
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR)
    tasks = [
        PlantLLLHVACEnv(seq_len=10,mode="flowering"),
        PlantLLLHVACEnv(seq_len=10,mode="seeding"),
        PlantLLLHVACEnv(seq_len=10,mode="growing"),
    ]
    #latent_dim = 32
    #action_dim=4
    #num_classes = 3
    batch_size = 32
    num_epochs_per_task = 20
    num_tasks = len(tasks) #"flowing, seeding, growing"
    plant_mode=0
    env = tasks[plant_mode]
    env_pipe_trainer(
        lll_model=env.lll_model ,
        num_tasks=num_tasks,
        num_classes=env.action_dim,
        num_epochs_per_task= num_epochs_per_task,
        batch_size=batch_size,
        learning_rate=0.001,
        ewc_lambda=0.4)

    # ç”Ÿæˆä»£è¡¨æ€§æ•°æ®
    
    representative_data, y_train    = generate_smart_representative_data(env, 100, return_labels=True)

    # ç¡®ä¿å½¢çŠ¶åŒ¹é…ï¼ˆå¦‚æœéœ€è¦åºåˆ—æ•°æ®ï¼‰
    if len(representative_data.shape) == 2:
        representative_data = representative_data.reshape(-1, 1, env.state_dim)
    agent = ESP32OnlinePPOFisherAgent(state_dim=env.state_dim, action_dim=env.action_dim, hidden_units=8)

    agent.compute_env_fisher_matrix(representative_data)
    # ä¿å­˜ Fisher & Optimal Params
    path_npz=os.path.join(MODEL_DIR, "env_fisher.npz")
    agent.save_fisher_and_params(path_npz)
    # ä¿å­˜ TFLite
    agent.save_tflite_model(filepath="esp32_actor.tflite" ,model_type='actor')
    agent.save_tflite_model(filepath="esp32_critic.tflite" ,model_type='critic')
    path_actor_h5 = os.path.join(MODEL_DIR, "esp32ppo_actor.h5")
    path_critic_h5 = os.path.join(MODEL_DIR, "esp32ppo_critic.h5")
    agent.actor.save (path_actor_h5)
    agent.critic.save(path_critic_h5)



    policy_agent=ESP32OnlinePPOFisherAgent(fisher_matrix=agent.fisher_matrix,optimal_params=agent.optimal_params)
    #trainbytask_lifelong_ppo(env,policy_agent,tasks=tasks[plant_mode])
    #policy_agent.learn(total_timesteps=1000000)
    #policy_agent.train_buffer_step(use_ewc=True)
    policy_agent.rollout_and_train(env=env )
    path_policy_h5 = os.path.join(MODEL_DIR, "esp32_OnlinePPOFisher.h5")
    policy_agent.actor.save(path_policy_h5)
    policy_agent.actor.summary()
    # 4. åˆ›å»ºå¯¼å‡ºå™¨å¹¶ç”ŸæˆOTAåŒ…
    exporter = TensorFlowESP32Exporter(path_policy_h5)
    path_policy_json = os.path.join(MODEL_DIR, "esp32_policy.json")

    # 5. ç”Ÿæˆå¹¶ä¿å­˜OTAåŒ…
    exporter.save_ota_package(
        output_path=path_policy_json,
        representative_data=representative_data,
        fine_tune_data=(representative_data, y_train),
        firmware_version="1.0.0",
        prune=True,  # å¯ç”¨å‰ªæ
        compress=False,
        quantize=False
    )
    #ota_package = exporter.create_ota_package(representative_data, quantize=True)
    ota_package=exporter.ota_package
    compressed_bytes = base64.b64decode(ota_package['model_data_b64'])

    # æ­£ç¡®çš„è§£å‹æ–¹å¼ï¼ˆç¡®ä¿ä½¿ç”¨zlibè§£å‹ï¼‰
    try:
        decompressed_bytes = zlib.decompress(compressed_bytes)
    except:
        # å¦‚æœè§£å‹å¤±è´¥ï¼Œå¯èƒ½æ•°æ®æ²¡æœ‰è¢«å‹ç¼©
        decompressed_bytes = compressed_bytes

    # éªŒè¯TFLiteæ¨¡å‹æœ‰æ•ˆæ€§
    try:
        # å°è¯•åŠ è½½æ¨¡å‹æ¥éªŒè¯
        interpreter = tf.lite.Interpreter(model_content=decompressed_bytes)
        interpreter.allocate_tensors()
        print("âœ“ TFLiteæ¨¡å‹éªŒè¯æˆåŠŸ")
    except Exception as e:
        print(f"âœ— TFLiteæ¨¡å‹æ— æ•ˆ: {e}")
        sys.exit(1)
        # éªŒè¯TFLiteæ¨¡å‹
    # ä½¿ç”¨
    #analyze_model_details(decompressed_bytes)
    # è·å–æ‰€æœ‰å¼ é‡è¯¦ç»†ä¿¡æ¯
    tensor_details = interpreter.get_tensor_details()
    print("TFLite æ¨¡å‹å®Œæ•´å±‚ä¿¡æ¯")
    print("=" * 80)
    print(f"{'ç´¢å¼•':<5} {'åç§°':<25} {'å½¢çŠ¶':<15} {'æ•°æ®ç±»å‹':<12} {'é‡åŒ–ä¿¡æ¯'}")
    print("=" * 80)

    for tensor in tensor_details:
        print(
            f"{tensor['index']:<5} {tensor['name']:<25} {str(tensor['shape']):<15} {str(tensor['dtype']):<12} {tensor.get('quantization', 'None')}")

    print("TFLite æ¨¡å‹å®Œæ•´å±‚ä¿¡æ¯")
    print("=" * 80)
    print(f"{'ç´¢å¼•':<5} {'åç§°':<25} {'å½¢çŠ¶':<15} {'æ•°æ®ç±»å‹':<12} {'é‡åŒ–ä¿¡æ¯'}")
    print("=" * 80)

    for tensor in tensor_details:
        # æ­£ç¡®å¤„ç†é‡åŒ–ä¿¡æ¯ï¼ˆå¯èƒ½æ˜¯å…ƒç»„æˆ–Noneï¼‰
        quantization = tensor.get('quantization', ())

        if quantization and isinstance(quantization, (list, tuple)) and len(quantization) >= 2:
            scale, zero_point = quantization[0], quantization[1]
            quant_info = f"scale:{scale}, zero_point:{zero_point}"
        else:
            quant_info = "quantization free"

        print(
            f"{tensor['index']:<5} {str(tensor['name'])[:24]:<25} {str(tensor['shape']):<15} {str(tensor['dtype']):<12} {quant_info}")

    print("=" * 80)
    print(f" æ€»å±‚æ•°: {len(tensor_details)}")
    print(f" æ¨¡å‹å¤§å°: {len(decompressed_bytes):,} å­—èŠ‚")
    path_policy_tflite = os.path.join(MODEL_DIR, "esp32_optimized_model.tflite")
    with open(path_policy_tflite, 'wb') as f:
        f.write(decompressed_bytes)

    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {path_policy_tflite}")
    print(f"æ¨¡å‹å¤§å°: {len(decompressed_bytes)} å­—èŠ‚")

    # ä½¿ç”¨Post-Training Quantizationè¿›è¡Œé‡åŒ–
    #model = tf.keras.models.load_model(path_policy_h5)

    #quantize_model = tf.quantization.quantize(model)

    # å°†é‡åŒ–åçš„æ¨¡å‹ä¿å­˜ä¸º TFLite æ ¼å¼
    #converter = tf.lite.TFLiteConverter.from_keras_model(quantize_model)
    #converter.optimizations = [tf.lite.Optimize.DEFAULT]

    # è®¾ç½®æ”¯æŒé‡åŒ–çš„æ“ä½œ
    #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    #converter.representative_dataset = representative_data
    #tflite_model = converter.convert()
    #with open(path_policy_tflite, 'wb') as f:
    #    f.write(tflite_model)
    # è¿è¡Œè°ƒè¯•
    debug_model_creation(representative_data)
    # åœ¨ä¿å­˜åç«‹å³éªŒè¯
    if verify_tflite_model(path_policy_tflite):
        print("PCç«¯éªŒè¯é€šè¿‡ï¼Œå¯ä»¥ä¸Šä¼ åˆ°ESP32")
    else:
        print("PCç«¯éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹")
    # ä¹Ÿå¯å–®ç¨å‘¼å«
    #_ = exporter.apply_quantization(representative_data)
    #_ = exporter.compute_fisher_matrix(representative_data)
