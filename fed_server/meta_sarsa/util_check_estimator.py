import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime

from sklearn.preprocessing import StandardScaler




def check_value_estimation(qmodel, encoder, files, num_samples=100):
    """
    æ£€æŸ¥å€¼å‡½æ•°ä¼°è®¡çš„è´¨é‡
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š å€¼å‡½æ•°ä¼°è®¡è´¨é‡æ£€æŸ¥")
    print("=" * 60)

    results = {
        'bellman_errors': [],
        'td_errors': [],
        'q_value_consistency': [],
        'value_drift': [],
        'estimation_bias': []
    }

    # 1. æ£€æŸ¥Bellmanä¸€è‡´æ€§
    print("1. ğŸ”„ Bellmanä¸€è‡´æ€§æ£€æŸ¥:")
    bellman_results = check_bellman_consistency(qmodel, encoder, files, num_samples)
    results['bellman_errors'].extend(bellman_results['errors'])

    # 2. æ£€æŸ¥Qå€¼ç¨³å®šæ€§
    print("\n2. ğŸ“ˆ Qå€¼ç¨³å®šæ€§æ£€æŸ¥:")
    stability_results = check_q_value_stability(qmodel, encoder, files)
    results['q_value_consistency'].extend(stability_results['consistency_scores'])

    # 3. æ£€æŸ¥å€¼æ¼‚ç§»
    print("\n3. ğŸ¯ å€¼æ¼‚ç§»æ£€æŸ¥:")
    drift_results = check_value_drift(qmodel, encoder, files)
    results['value_drift'].extend(drift_results['drift_values'])

    # 4. æ£€æŸ¥ä¼°è®¡åå·®
    print("\n4. âš–ï¸ ä¼°è®¡åå·®æ£€æŸ¥:")
    bias_results = check_estimation_bias(qmodel, encoder, files)
    results['estimation_bias'].extend(bias_results['bias_scores'])

    # 5. ç»¼åˆåˆ†æ
    print("\n5. ğŸ“‹ ç»¼åˆåˆ†æç»“æœ:")
    analyze_estimation_quality(results)

    return results
def standardize_row(row):
    mean = np.mean(row)
    std = np.std(row)
    if std == 0:
        return np.zeros_like(row) # Avoid division by zero
    return (row - mean) / std

def normalize_row(row):
    norm = np.linalg.norm(row) # L2 norm by default
    if norm == 0:
        return row # Avoid division by zero
    return row / norm
def check_bellman_consistency(qmodel, encoder, files, num_samples=50):
    """
    æ£€æŸ¥Bellmanæ–¹ç¨‹çš„ä¸€è‡´æ€§
    """
    print("  è®¡ç®—Bellmanè¯¯å·®...")

    errors = []
    td_errors = []

    # éšæœºé€‰æ‹©æ–‡ä»¶å’Œæ•°æ®ç‚¹
    for _ in range(num_samples):
        df = pd.read_csv(np.random.choice(files))
        if len(df) < 12:  # éœ€è¦è‡³å°‘12ä¸ªç‚¹ï¼ˆå½“å‰+ä¸‹ä¸€ä¸ª+10çª—å£ï¼‰
            continue

        # éšæœºé€‰æ‹©èµ·å§‹ç‚¹
        start_idx = np.random.randint(0, len(df) - 11)

        # æ„å»ºå½“å‰çŠ¶æ€ï¼ˆ10ä¸ªæ—¶é—´æ­¥çš„çª—å£ï¼‰
        current_states = []
        for i in range(10):
            row = df.iloc[start_idx + i][["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values
            current_states.append(normalize_row(row))
        current_state = np.array(current_states)

        # ç¼–ç å½“å‰çŠ¶æ€
        current_encoded = encoder(current_state[np.newaxis, :, :]).numpy()[0]

        # è·å–å½“å‰Qå€¼
        current_q_values = qmodel.q_net.predict(current_encoded.reshape(1, -1), verbose=0)[0]
        current_action = np.argmax(current_q_values)
        current_max_q = np.max(current_q_values)

        # è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€
        next_idx = start_idx + 10
        next_row = df.iloc[next_idx][["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values
        next_row = normalize_row(next_row)

        # ç”¨å½“å‰åŠ¨ä½œæ›¿æ¢å¼€å…³çŠ¶æ€
        action_bits = qmodel.action_int_to_bits(current_action)
        next_row[3:7] = action_bits

        # æ„å»ºä¸‹ä¸€ä¸ªçŠ¶æ€çª—å£ï¼ˆç§»é™¤æœ€æ—§ï¼Œæ·»åŠ æœ€æ–°ï¼‰
        next_states = np.vstack([current_state[1:], next_row])
        next_encoded = encoder(next_states[np.newaxis, :, :]).numpy()[0]

        # è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„Qå€¼
        next_q_values = qmodel.q_net.predict(next_encoded.reshape(1, -1), verbose=0)[0]
        next_max_q = np.max(next_q_values)

        # è®¡ç®—å¥–åŠ±
        label = df.iloc[next_idx]["label"] if "label" in df.columns else 0
        reward = qmodel.compute_reward_batch(np.array([label]), np.array([action_bits]))[0]

        # è®¡ç®—Bellmanè¯¯å·®
        bellman_estimate = reward + qmodel.gamma * next_max_q
        bellman_error = abs(current_max_q - bellman_estimate)
        td_error = current_max_q - bellman_estimate

        errors.append(bellman_error)
        td_errors.append(td_error)

    print(f"  å¹³å‡Bellmanè¯¯å·®: {np.mean(errors):.4f} Â± {np.std(errors):.4f}")
    print(f"  å¹³å‡TDè¯¯å·®: {np.mean(td_errors):.4f} Â± {np.std(td_errors):.4f}")

    return {'errors': errors, 'td_errors': td_errors}


def check_q_value_stability(qmodel, encoder, files, num_tests=20):
    """
    æ£€æŸ¥Qå€¼çš„ç¨³å®šæ€§ï¼ˆå¤šæ¬¡ä¼°è®¡çš„ä¸€è‡´æ€§ï¼‰
    """
    print("  æ£€æŸ¥Qå€¼ä¼°è®¡ç¨³å®šæ€§...")

    consistency_scores = []

    for _ in range(num_tests):
        df = pd.read_csv(np.random.choice(files))
        if len(df) < 10:
            continue

        # é€‰æ‹©éšæœºçŠ¶æ€
        start_idx = np.random.randint(0, len(df) - 9)
        states = []
        for i in range(10):
            row = df.iloc[start_idx + i][["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values
            states.append(normalize_row(row))

        state = np.array(states)
        state_encoded = encoder(state[np.newaxis, :, :]).numpy()[0]

        # å¤šæ¬¡é¢„æµ‹åŒä¸€ä¸ªçŠ¶æ€ï¼Œæ£€æŸ¥ä¸€è‡´æ€§
        predictions = []
        for _ in range(5):  # é¢„æµ‹5æ¬¡
            q_values = qmodel.q_net.predict(state_encoded.reshape(1, -1), verbose=0)[0]
            predictions.append(q_values)

        # è®¡ç®—é¢„æµ‹ä¹‹é—´çš„æ ‡å‡†å·®
        pred_array = np.array(predictions)
        consistency = np.mean(np.std(pred_array, axis=0))  # å¹³å‡æ ‡å‡†å·®
        consistency_scores.append(consistency)

    avg_consistency = np.mean(consistency_scores)
    print(f"  Qå€¼ä¼°è®¡ç¨³å®šæ€§: {avg_consistency:.6f}")

    if avg_consistency > 0.1:
        print("  âš ï¸  è­¦å‘Š: Qå€¼ä¼°è®¡ä¸ç¨³å®š")
    else:
        print("  âœ… Qå€¼ä¼°è®¡ç¨³å®š")

    return {'consistency_scores': consistency_scores}


def check_value_drift(qmodel, encoder, files, num_sequences=10):
    """
    æ£€æŸ¥å€¼å‡½æ•°æ¼‚ç§»ï¼ˆéšæ—¶é—´çš„å˜åŒ–ï¼‰
    """
    print("  æ£€æŸ¥å€¼å‡½æ•°æ¼‚ç§»...")

    drift_values = []

    for _ in range(num_sequences):
        df = pd.read_csv(np.random.choice(files))
        if len(df) < 50:
            continue

        # è·Ÿè¸ªä¸€ä¸ªåºåˆ—ä¸­çš„Qå€¼å˜åŒ–
        q_values_over_time = []

        # åˆ›å»ºåˆå§‹çŠ¶æ€çª—å£
        states = []
        for i in range(10):
            row = df.iloc[i][["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values
            states.append(normalize_row(row))

        for t in range(10, min(50, len(df))):
            # ç¼–ç å½“å‰çŠ¶æ€
            current_state = np.array(states)
            state_encoded = encoder(current_state[np.newaxis, :, :]).numpy()[0]

            # è·å–Qå€¼
            q_values = qmodel.q_net.predict(state_encoded.reshape(1, -1), verbose=0)[0]
            max_q = np.max(q_values)
            q_values_over_time.append(max_q)

            # é€‰æ‹©åŠ¨ä½œå¹¶æ›´æ–°çŠ¶æ€
            action = np.argmax(q_values)
            action_bits = qmodel.action_int_to_bits(action)

            # è·å–ä¸‹ä¸€ä¸ªè§‚æµ‹
            next_row = df.iloc[t][["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values
            next_row = normalize_row(next_row)
            next_row[3:7] = action_bits

            # æ›´æ–°çŠ¶æ€çª—å£
            states = states[1:] + [next_row]

        # è®¡ç®—æ¼‚ç§»ï¼ˆå¼€å§‹å’Œç»“æŸçš„å·®å¼‚ï¼‰
        if len(q_values_over_time) > 1:
            drift = abs(q_values_over_time[-1] - q_values_over_time[0])
            drift_values.append(drift)

    avg_drift = np.mean(drift_values) if drift_values else 0
    print(f"  å¹³å‡å€¼æ¼‚ç§»: {avg_drift:.4f}")

    return {'drift_values': drift_values}


def check_estimation_bias(qmodel, encoder, files, num_comparisons=30):
    """
    æ£€æŸ¥ä¼°è®¡åå·®ï¼ˆä¸ç†æƒ³å€¼çš„æ¯”è¾ƒï¼‰
    """
    print("  æ£€æŸ¥ä¼°è®¡åå·®...")

    bias_scores = []

    # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¸€äº›"ç†æƒ³"çš„å‚è€ƒå€¼
    # ç”±äºæ²¡æœ‰çœŸå®å€¼ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€äº›å¯å‘å¼æ–¹æ³•

    for _ in range(num_comparisons):
        df = pd.read_csv(np.random.choice(files))
        if len(df) < 20:
            continue

        # é€‰æ‹©çŠ¶æ€
        idx = np.random.randint(10, len(df) - 10)
        states = []
        for i in range(10):
            row = df.iloc[idx - 9 + i][["temp", "humid", "light", "ac", "heater", "dehum", "hum"]].values
            states.append(normalize_row(row))

        state = np.array(states)
        state_encoded = encoder(state[np.newaxis, :, :]).numpy()[0]

        # æ¨¡å‹é¢„æµ‹çš„Qå€¼
        model_q_values = qmodel.q_net.predict(state_encoded.reshape(1, -1), verbose=0)[0]
        model_max_q = np.max(model_q_values)

        # å¯å‘å¼å‚è€ƒå€¼ï¼ˆåŸºäºçŠ¶æ€ç‰¹å¾ï¼‰
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„å¯å‘å¼ï¼šæ¸©åº¦ã€æ¹¿åº¦è¶Šæ¥è¿‘0.5ï¼Œä»·å€¼è¶Šé«˜
        avg_temp = np.mean(state[:, 0])
        avg_humid = np.mean(state[:, 1])

        # ç†æƒ³å€¼ï¼ˆ0.5æ˜¯æœ€ä½³çŠ¶æ€ï¼‰
        ideal_value = 1.0 - (abs(avg_temp - 0.5) + abs(avg_humid - 0.5)) / 2.0

        # è®¡ç®—åå·®
        bias = abs(model_max_q - ideal_value)
        bias_scores.append(bias)

    avg_bias = np.mean(bias_scores)
    print(f"  å¹³å‡ä¼°è®¡åå·®: {avg_bias:.4f}")

    if avg_bias > 0.3:
        print("  âš ï¸  è­¦å‘Š: ä¼°è®¡åå·®è¾ƒå¤§")
    elif avg_bias > 0.1:
        print("  â„¹ï¸  ä¼°è®¡åå·®ä¸­ç­‰")
    else:
        print("  âœ… ä¼°è®¡åå·®è¾ƒå°")

    return {'bias_scores': bias_scores}


def analyze_estimation_quality(results):
    """
    ç»¼åˆåˆ†æå€¼å‡½æ•°ä¼°è®¡è´¨é‡
    """
    bellman_errors = results['bellman_errors']
    consistency_scores = results['q_value_consistency']
    drift_values = results['value_drift']
    bias_scores = results['estimation_bias']

    print("  ğŸ“Š å€¼å‡½æ•°ä¼°è®¡è´¨é‡æŠ¥å‘Š:")
    print("  " + "-" * 40)

    if bellman_errors:
        avg_bellman = np.mean(bellman_errors)
        print(f"  Bellmanä¸€è‡´æ€§: {avg_bellman:.4f}")
        if avg_bellman > 0.5:
            print("    âŒ Bellmanè¯¯å·®è¿‡å¤§ - å€¼å‡½æ•°ä¼°è®¡ä¸å‡†ç¡®")
        elif avg_bellman > 0.2:
            print("    âš ï¸  Bellmanè¯¯å·®ä¸­ç­‰ - éœ€è¦æ”¹è¿›")
        else:
            print("    âœ… Bellmanä¸€è‡´æ€§è‰¯å¥½")

    if consistency_scores:
        avg_consistency = np.mean(consistency_scores)
        print(f"  ä¼°è®¡ç¨³å®šæ€§: {avg_consistency:.6f}")
        if avg_consistency > 0.05:
            print("    âŒ ä¼°è®¡ä¸ç¨³å®š - ç½‘ç»œè®­ç»ƒæœ‰é—®é¢˜")
        elif avg_consistency > 0.01:
            print("    âš ï¸  ç¨³å®šæ€§ä¸€èˆ¬ - å¯èƒ½æœ‰æ³¢åŠ¨")
        else:
            print("    âœ… ä¼°è®¡ç¨³å®š")

    if drift_values:
        avg_drift = np.mean(drift_values)
        print(f"  å€¼æ¼‚ç§»: {avg_drift:.4f}")
        if avg_drift > 0.3:
            print("    âŒ å€¼æ¼‚ç§»ä¸¥é‡ - å­¦ä¹ è¿‡ç¨‹ä¸ç¨³å®š")
        elif avg_drift > 0.1:
            print("    âš ï¸  ä¸­ç­‰å€¼æ¼‚ç§»")
        else:
            print("    âœ… å€¼æ¼‚ç§»æ§åˆ¶è‰¯å¥½")

    if bias_scores:
        avg_bias = np.mean(bias_scores)
        print(f"  ä¼°è®¡åå·®: {avg_bias:.4f}")
        if avg_bias > 0.3:
            print("    âŒ ä¼°è®¡åå·®è¿‡å¤§")
        elif avg_bias > 0.1:
            print("    âš ï¸  ä¸­ç­‰ä¼°è®¡åå·®")
        else:
            print("    âœ… ä¼°è®¡åå·®è¾ƒå°")

    # æ€»ä½“è¯„ä¼°
    overall_score = 0
    max_score = 4

    if bellman_errors and np.mean(bellman_errors) < 0.2:
        overall_score += 1
    if consistency_scores and np.mean(consistency_scores) < 0.01:
        overall_score += 1
    if drift_values and np.mean(drift_values) < 0.1:
        overall_score += 1
    if bias_scores and np.mean(bias_scores) < 0.1:
        overall_score += 1

    quality_rating = overall_score / max_score
    print(f"  ğŸ¯ æ€»ä½“è´¨é‡è¯„åˆ†: {quality_rating:.1%}")

    if quality_rating >= 0.75:
        print("    âœ… å€¼å‡½æ•°ä¼°è®¡è´¨é‡ä¼˜ç§€")
    elif quality_rating >= 0.5:
        print("    âš ï¸  å€¼å‡½æ•°ä¼°è®¡è´¨é‡ä¸€èˆ¬")
    else:
        print("    âŒ å€¼å‡½æ•°ä¼°è®¡è´¨é‡è¾ƒå·®")


# ä½¿ç”¨ç¤ºä¾‹
def run_value_estimation_check(qmodel, encoder, files):
    """
    è¿è¡Œå€¼å‡½æ•°ä¼°è®¡æ£€æŸ¥
    """
    # å‡è®¾ä½ å·²ç»æœ‰äº†è¿™äº›ç»„ä»¶
    # qmodel = YourDQNAgent(...)
    # encoder = YourEncoder(...)
    # files = ["data1.csv", "data2.csv"]

    print("å¼€å§‹å€¼å‡½æ•°ä¼°è®¡æ£€æŸ¥...")
    results = check_value_estimation(qmodel, encoder, files, num_samples=50)

    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_df = pd.DataFrame({
        'bellman_errors': results['bellman_errors'],
        'consistency_scores': results['q_value_consistency'],
        'value_drift': results['value_drift'],
        'estimation_bias': results['estimation_bias']
    })
    results_df.to_csv(f'value_estimation_analysis_{timestamp}.csv', index=False)

    return results

# è¿è¡Œæ£€æŸ¥
# value_estimation_results = run_value_estimation_check()