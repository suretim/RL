# server_side_processor_tf.py
import tensorflow as tf
from tensorflow import keras
import numpy as np
import zlib
import sys
import json
import os
import base64
from datetime import datetime
import tensorflow_model_optimization as tfmot
from typing import Dict, Any, Optional, Union

# ä½ å°ˆæ¡ˆå…§çš„é¡ï¼ˆä¿æŒä¸å‹•ï¼Œå‡è¨­å­˜åœ¨ï¼‰
from util_exporter import TensorFlowESP32Exporter
from util_hvac_agent import ESP32OnlinePPOFisherAgent,ESP32PPOFisherAgent
from util_env import PlantLLLHVACEnv

MODEL_DIR = "./models"

def ewc_update(actor, fisher, optimal_params, current_params, learning_rate=1e-3, ewc_lambda=500):
    """
    ä½¿ç”¨ Fisher çŸ©é™£æ›´æ–° Actor åƒæ•¸
    actor: tf.keras.Sequential
    fisher: dict
    optimal_params: dict
    current_params: dict, å¯ç”¨ actor.trainable_variables
    """
    with tf.GradientTape() as tape:
        loss = 0.0
        for var in actor.trainable_variables:
            name = var.name
            diff = var - optimal_params[name]
            loss += tf.reduce_sum(fisher[name] * diff ** 2)
        loss *= ewc_lambda

    grads = tape.gradient(loss, actor.trainable_variables)
    for var, g in zip(actor.trainable_variables, grads):
        if g is not None:
            var.assign_sub(learning_rate * g)

import tensorflow_probability as tfp
import os

tfd = tfp.distributions


class xESP32PPOAgent:
    """å°ˆç‚ºESP32è¨­è¨ˆçš„è¼•é‡ç´šPPOä»£ç†"""

    def __init__(self, state_dim=5, action_dim=4,
                 clip_epsilon=0.2, value_coef=0.5, entropy_coef=0.01):
        # é€™è£¡ç°¡å–®å»ºä¸€å€‹ policy (MLP)
        inputs = keras.Input(shape=(state_dim,))
        x = keras.layers.Dense(32, activation="relu")(inputs)
        outputs = keras.layers.Dense(action_dim, activation="softmax")(x)
        self.policy = keras.Model(inputs, outputs)

        # å…¶å®ƒ PPO è¶…åƒæ•¸
        self.clip_epsilon = clip_epsilon
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef

    def act(self, state):
        """è¼¸å…¥ state -> è¼¸å‡º action"""
        state = tf.convert_to_tensor([state], dtype=tf.float32)
        probs = self.policy(state)
        action = tf.random.categorical(tf.math.log(probs), num_samples=1)
        return int(action[0, 0])

    def save_model(self, path="trained_policy_tf.keras"):
        """ä¿å­˜ policy æ¨¡å‹"""
        self.policy.save(path)
        print(f" æ¨¡å‹å·²ä¿å­˜åˆ° {path}")

    def load_model(self, path="trained_policy_tf.keras"):
        """è¼‰å…¥ policy æ¨¡å‹"""
        self.policy = keras.models.load_model(path)
        print(f" æ¨¡å‹å·²å¾ {path} è¼‰å…¥")

def generate_smart_representative_data(env, num_samples=1000, mode_weights=None, return_labels=False):
    """
    æ™ºèƒ½ç”Ÿæˆä»£è¡¨æ€§æ•°æ®ï¼Œè¦†ç›–ä¸åŒæ¨¡å¼å’Œå·¥å†µ

    Args:
        env: PlantLLLHVACEnvå®ä¾‹
        num_samples: æ€»æ ·æœ¬æ•°
        mode_weights: ä¸åŒæ¨¡å¼çš„æƒé‡ [growing, flowering, seeding]
        return_labels: æ˜¯å¦åŒæ—¶è¿”å›æ ‡ç­¾ (X, y)
    """
    if mode_weights is None:
        mode_weights = [0.4, 0.3, 0.3]  # é»˜è®¤æƒé‡

    all_data = []
    all_labels = []

    # ä¸ºæ¯ä¸ªæ¨¡å¼ç”Ÿæˆæ•°æ®
    modes = ["growing", "flowering", "seeding"]

    for mode, weight in zip(modes, mode_weights):
        num_mode_samples = int(num_samples * weight)
        print(f"ä¸ºæ¨¡å¼ '{mode}' ç”Ÿæˆ {num_mode_samples} ä¸ªæ ·æœ¬")

        # è®¾ç½®ç¯å¢ƒæ¨¡å¼
        env.mode = mode
        env.reset()

        # ç”Ÿæˆè¯¥æ¨¡å¼çš„æ•°æ®
        for i in range(num_mode_samples):
            if mode == "growing":
                action = np.random.choice([0, 1], size=4, p=[0.3, 0.7])
            elif mode == "flowering":
                action = np.random.choice([0, 1], size=4, p=[0.5, 0.5])
            else:  # seeding
                action = np.random.choice([0, 1], size=4, p=[0.7, 0.3])

            # æ‰§è¡ŒåŠ¨ä½œ
            true_label = modes.index(mode)
            next_state, reward, done, info = env.step(action, true_label=true_label)

            # æ·»åŠ æ•°æ®ç‚¹ & æ ‡ç­¾
            current_data_point = env.current_sequence[0, -1]
            all_data.append(current_data_point)
            all_labels.append(true_label)

            if done:
                env.reset()

    # è½¬ numpy
    all_data = np.array(all_data, dtype=np.float32)
    all_labels = np.array(all_labels, dtype=np.int32)

    # éšæœºæ‰“ä¹±
    idx = np.arange(len(all_data))
    np.random.shuffle(idx)
    all_data, all_labels = all_data[idx], all_labels[idx]

    if return_labels:
        return all_data[:num_samples], all_labels[:num_samples]
    else:
        return all_data[:num_samples]
from util_trainer import LLLTrainer

def env_pipe_trainer(lll_model=None,num_tasks=3,latent_dim=64,num_classes=3,num_epochs_per_task=3,batch_size=32, learning_rate=0.001, ewc_lambda=0.4):
    #env_lll_model, state_dim, action_dim, hidden_units,learning_rate=0.001, ewc_lambda=0.4

    trainer = LLLTrainer(lll_model=lll_model, learning_rate=0.001, ewc_lambda=0.4)

    # æ¨¡æ‹Ÿå¤šä»»åŠ¡æ•°æ®
    for task_id in range(num_tasks):
        print(f"\n=== Training Task {task_id + 1} ===")
        num_samples = 200
        latent_features = np.random.randn(num_samples, latent_dim).astype(np.float32)
        labels = np.random.randint(0, num_classes, size=(num_samples,)).astype(np.int32)
        loss=0
        # æŒ‰ batch è®­ç»ƒå½“å‰ä»»åŠ¡
        for epoch in range(num_epochs_per_task):
            indices = np.random.permutation(num_samples)
            latent_features_shuffled = latent_features[indices]
            labels_shuffled = labels[indices]

            for start_idx in range(0, num_samples, batch_size):
                end_idx = start_idx + batch_size
                batch_latent = latent_features_shuffled[start_idx:end_idx]
                batch_labels = labels_shuffled[start_idx:end_idx]
                loss = trainer._train_lll_model(batch_latent, batch_labels)
            print(f"  Epoch {epoch + 1}, Last batch loss: {loss:.4f}")

        # è®­ç»ƒå®Œå½“å‰ä»»åŠ¡åï¼Œæ›´æ–° EWC ä¿¡æ¯
        trainer.update_ewc(latent_features, labels)


def verify_tflite_model(file_path):
    """æ­£ç¡®çš„TFLiteæ¨¡å‹éªŒè¯å‡½æ•°"""
    try:
        with open(file_path, 'rb') as f:
            model_data = f.read()

        if len(model_data) < 16:
            print(" æ–‡ä»¶å¤ªå°")
            return False

        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {len(model_data)} å­—èŠ‚")
        print(f"ğŸ“Š å‰16å­—èŠ‚: {model_data[:16].hex(' ')}")

        # æ­£ç¡®çš„TFLiteé­”æœ¯æ•°å­—æ£€æµ‹ï¼ˆä»ç¬¬5å­—èŠ‚å¼€å§‹ï¼‰
        if len(model_data) >= 8:
            if model_data[4:8] == b'TFL3':
                print(" æ£€æµ‹åˆ°æœ‰æ•ˆçš„TFLiteé­”æœ¯æ•°å­— (TFL3)")
                magic_ok = True
            else:
                print(f" æ— æ•ˆçš„TFLiteé­”æœ¯æ•°å­—: {model_data[4:8].hex(' ')}")
                magic_ok = False
        else:
            magic_ok = False

        # ç”¨TensorFlowéªŒè¯ï¼ˆè¿™æ˜¯æœ€å¯é çš„ï¼‰
        try:
            interpreter = tf.lite.Interpreter(model_content=model_data)
            interpreter.allocate_tensors()
            print(" TensorFlowéªŒè¯æˆåŠŸ")
            tf_ok = True
        except Exception as e:
            print(f" TensorFlowéªŒè¯å¤±è´¥: {e}")
            tf_ok = False

        # æœ€ç»ˆç»“æœï¼šåªè¦TensorFlowéªŒè¯æˆåŠŸå°±è®¤ä¸ºæœ‰æ•ˆ
        if tf_ok:
            if not magic_ok:
                print("ï¸  æ³¨æ„ï¼šæ–‡ä»¶ç»“æ„ç‰¹æ®Šï¼Œä½†TensorFlowå¯ä»¥åŠ è½½")
            return True
        else:
            return False

    except Exception as e:
        print(f" æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return False

def debug_model_creation(representative_data):
    print("ğŸ” å¼€å§‹è¯¦ç»†è°ƒè¯•æ¨¡å‹åˆ›å»ºè¿‡ç¨‹...")

    # 1. åˆ›å»ºOTAåŒ…
    ota_package = exporter.create_ota_package(representative_data, quantize=True)
    print(f"âœ… OTAåŒ…åˆ›å»ºæˆåŠŸ")
    print(f"OTAåŒ…é”®å€¼: {list(ota_package.keys())}")

    # 2. æ£€æŸ¥model_data_b64
    model_data_b64 = ota_package['model_data_b64']
    print(f"Base64æ•°æ®é•¿åº¦: {len(model_data_b64)} å­—ç¬¦")
    print(f"Base64å‰50å­—ç¬¦: {repr(model_data_b64[:50])}")

    # 3. Base64è§£ç 
    try:
        raw_bytes = base64.b64decode(model_data_b64)
        print(f"âœ… Base64è§£ç æˆåŠŸ")
        print(f"è§£ç åå¤§å°: {len(raw_bytes)} å­—èŠ‚")
        print(f"å‰32å­—èŠ‚: {raw_bytes[:32].hex(' ')}")
    except Exception as e:
        print(f"âŒ Base64è§£ç å¤±è´¥: {e}")
        return False

    # 4. åˆ†ææ•°æ®æ ¼å¼
    print("\nğŸ” æ•°æ®åˆ†æ:")

    # æ£€æŸ¥æ˜¯å¦æ˜¯TFLiteæ ¼å¼ - ä¿®æ­£åçš„é€»è¾‘
    if len(raw_bytes) >= 8:  # éœ€è¦è‡³å°‘8å­—èŠ‚æ‰èƒ½æ£€æŸ¥çœŸæ­£çš„é­”æœ¯æ•°å­—
        # çœŸæ­£çš„TFLiteé­”æœ¯æ•°å­—åœ¨ç¬¬5-8å­—èŠ‚ï¼ˆç´¢å¼•4-7ï¼‰
        magic = raw_bytes[4:8]
        if magic == b'TFL3':
            print("âœ… æ£€æµ‹åˆ°æœ‰æ•ˆçš„TFLiteé­”æœ¯æ•°å­— (TFL3)")
            print(f"é­”æœ¯æ•°å­—ä½ç½®: å­—èŠ‚4-7: {magic.hex(' ')}")
        else:
            print(f"âŒ ä¸æ˜¯TFLiteæ ¼å¼: å­—èŠ‚4-7 = {magic.hex(' ')}")
            print(f"æœŸæœ›: 54 46 4C 33 (TFL3)")

        # æ˜¾ç¤ºå®Œæ•´çš„å‰16å­—èŠ‚ç”¨äºè°ƒè¯•
        print(f"å‰16å­—èŠ‚å®Œæ•´æ•°æ®: {raw_bytes[:16].hex(' ')}")
        print(f"å­—èŠ‚0-3: {raw_bytes[:4].hex(' ')} (FlatBufferå¤´)")
        print(f"å­—èŠ‚4-7: {raw_bytes[4:8].hex(' ')} (TFLiteé­”æœ¯æ•°å­—)")
        print(f"å­—èŠ‚8-15: {raw_bytes[8:16].hex(' ')} (å…¶ä»–å…ƒæ•°æ®)")
    else:
        print("âŒ æ•°æ®å¤ªçŸ­ï¼Œæ— æ³•æ£€æŸ¥TFLiteé­”æœ¯æ•°å­—")

    # æ£€æŸ¥æ˜¯å¦æ˜¯å…¶ä»–æ ¼å¼
    common_formats = {
        b'\x78\x9C': 'Zlibå‹ç¼©',
        b'\x1F\x8B': 'Gzipå‹ç¼©',
        b'PK': 'Zipå‹ç¼©',
        b'{': 'JSONæ ¼å¼',
        b'<': 'XMLæ ¼å¼',
    }

    for magic, format_name in common_formats.items():
        if raw_bytes.startswith(magic):
            print(f"âš ï¸  æ£€æµ‹åˆ°å¯èƒ½æ˜¯ {format_name}")

    # 5. å°è¯•ç›´æ¥éªŒè¯
    try:
        interpreter = tf.lite.Interpreter(model_content=raw_bytes)
        interpreter.allocate_tensors()
        print("âœ… ç›´æ¥éªŒè¯æˆåŠŸ - æ•°æ®å·²ç»æ˜¯TFLiteæ ¼å¼")

        # ä¿å­˜æ–‡ä»¶
        path = os.path.join(MODEL_DIR, "esp32_optimized_model.tflite")
        with open(path, 'wb') as f:
            f.write(raw_bytes)
        print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {path}")
        return True

    except Exception as e:
        print(f"âŒ ç›´æ¥éªŒè¯å¤±è´¥: {e}")

    # 6. å°è¯•ä½œä¸ºæ–‡æœ¬è§£ç ï¼ˆå¯èƒ½æ˜¯é”™è¯¯ä¿¡æ¯ï¼‰
    try:
        text_content = raw_bytes.decode('utf-8')
        print(f"âš ï¸  æ•°æ®å¯ä»¥è§£ç ä¸ºæ–‡æœ¬:")
        print(f"æ–‡æœ¬å†…å®¹: {text_content[:200]}...")

        # å¦‚æœæ˜¯JSONï¼Œå°è¯•è§£æ
        if text_content.strip().startswith('{'):
            import json
            try:
                json_data = json.loads(text_content)
                print(f"âœ… æ˜¯JSONæ ¼å¼: {list(json_data.keys())}")
            except:
                print("âŒ ä¸æ˜¯æœ‰æ•ˆçš„JSON")

    except UnicodeDecodeError:
        print("âš ï¸  æ•°æ®ä¸æ˜¯æ–‡æœ¬æ ¼å¼")

    return False


def analyze_model_details(model_bytes):
    """è¯¦ç»†åˆ†ææ¨¡å‹å†…å®¹"""
    interpreter0 = tf.lite.Interpreter(model_content=model_bytes)
    interpreter0.allocate_tensors()

    print("=== æ¨¡å‹è¯¦ç»†ä¿¡æ¯ ===")
    print(f"TensorFlow Liteç‰ˆæœ¬: {tf.__version__}")

    # è·å–æ‰€æœ‰æ“ä½œç¬¦
    print("\n=== æ“ä½œç¬¦åˆ—è¡¨ ===")
    for i, op in enumerate(interpreter0._get_ops_details()):
        print(f"{i}: {op['op_name']} (index: {op['index']})")

    # è¾“å…¥è¾“å‡ºè¯¦æƒ…
    print("\n=== è¾“å…¥å¼ é‡ ===")
    for i, detail in enumerate(interpreter0.get_input_details()):
        print(f"Input {i}: {detail['name']} {detail['shape']} {detail['dtype']}")

    print("\n=== è¾“å‡ºå¼ é‡ ===")
    for i, detail in enumerate(interpreter0.get_output_details()):
        print(f"Output {i}: {detail['name']} {detail['shape']} {detail['dtype']}")

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸æ”¯æŒçš„æ“ä½œç¬¦
    micro_supported_ops = ['FULLY_CONNECTED', 'SOFTMAX', 'RESHAPE', 'QUANTIZE', 'DEQUANTIZE']
    all_ops = [op['op_name'] for op in interpreter0._get_ops_details()]

    print("\n=== å…¼å®¹æ€§æ£€æŸ¥ ===")
    for op in all_ops:
        if op not in micro_supported_ops:
            print(f"ï¸  å¯èƒ½ä¸æ”¯æŒçš„æ“ä½œç¬¦: {op}")
        else:
            print(f" æ”¯æŒçš„æ“ä½œç¬¦: {op}")



# -----------------------------
# ä½¿ç”¨ç¯„ä¾‹ï¼ˆä¿®æ­£æˆå¯ load çš„å®Œæ•´æ¨¡å‹æª”ï¼‰
# -----------------------------
if __name__ == "__main__":
    # ä»£è¡¨æ€§è³‡æ–™ï¼ˆè«‹æ›¿æ›æˆä½ çš„å¯¦éš›è³‡æ–™ï¼‰
    # åˆ›å»ºç¯å¢ƒ
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR)
    latent_dim = 64
    action_dim=4
    num_classes = 3
    batch_size = 32
    num_epochs_per_task = 3
    num_tasks = 3
    env = PlantLLLHVACEnv(seq_len=10,   mode="growing")
    env_pipe_trainer(
        lll_model=env.lll_model ,
        num_tasks=num_tasks,
        latent_dim=latent_dim,
        num_classes=num_classes,
        num_epochs_per_task= num_epochs_per_task,
        batch_size=batch_size,
        learning_rate=0.001,
        ewc_lambda=0.4)

    # ç”Ÿæˆä»£è¡¨æ€§æ•°æ®
    #representative_data = generate_smart_representative_data(env, num_samples=1000)
    representative_data, y_train    = generate_smart_representative_data(env, 100, return_labels=True)

    # ç¡®ä¿å½¢çŠ¶åŒ¹é…ï¼ˆå¦‚æœéœ€è¦åºåˆ—æ•°æ®ï¼‰
    if len(representative_data.shape) == 2:
        representative_data = representative_data.reshape(-1, 1, 5)
    agent = ESP32PPOFisherAgent(state_dim=env.n_features, action_dim=action_dim, hidden_units=8)

    agent.compute_fisher_matrix(representative_data)
    # ä¿å­˜ Fisher & Optimal Params
    path_npz=os.path.join(MODEL_DIR, "esp32_fisher.npz")
    agent.save_fisher_and_params(path_npz)
    # ä¿å­˜ TFLite
    #agent.save_tflite_model("esp32_actor.tflite" )
    path_h5 = os.path.join(MODEL_DIR, "esp32ppo_actor.h5")
    agent.actor.save(path_h5)



    policy_agent=ESP32OnlinePPOFisherAgent(fisher_matrix=agent.fisher_matrix,optimal_params=agent.optimal_params)
    path_policy_h5 = os.path.join(MODEL_DIR, "esp32_OnlinePPOFisher.h5")
    policy_agent.actor.save(path_policy_h5)
    policy_agent.actor.summary()
    # 4. åˆ›å»ºå¯¼å‡ºå™¨å¹¶ç”ŸæˆOTAåŒ…
    exporter = TensorFlowESP32Exporter(path_policy_h5)
    path_policy_json = os.path.join(MODEL_DIR, "esp32_policy.json")

    # 5. ç”Ÿæˆå¹¶ä¿å­˜OTAåŒ…
    exporter.save_ota_package(
        output_path=path_policy_json,
        representative_data=representative_data,
        fine_tune_data=(representative_data, y_train),
        firmware_version="1.0.0",
        prune=True,  # å¯ç”¨å‰ªæ
        compress=False,
        quantize=False
    )
    #ota_package = exporter.create_ota_package(representative_data, quantize=True)
    ota_package=exporter.ota_package
    compressed_bytes = base64.b64decode(ota_package['model_data_b64'])

    # æ­£ç¡®çš„è§£å‹æ–¹å¼ï¼ˆç¡®ä¿ä½¿ç”¨zlibè§£å‹ï¼‰
    try:
        decompressed_bytes = zlib.decompress(compressed_bytes)
    except:
        # å¦‚æœè§£å‹å¤±è´¥ï¼Œå¯èƒ½æ•°æ®æ²¡æœ‰è¢«å‹ç¼©
        decompressed_bytes = compressed_bytes

    # éªŒè¯TFLiteæ¨¡å‹æœ‰æ•ˆæ€§
    try:
        # å°è¯•åŠ è½½æ¨¡å‹æ¥éªŒè¯
        interpreter = tf.lite.Interpreter(model_content=decompressed_bytes)
        interpreter.allocate_tensors()
        print("âœ“ TFLiteæ¨¡å‹éªŒè¯æˆåŠŸ")
    except Exception as e:
        print(f"âœ— TFLiteæ¨¡å‹æ— æ•ˆ: {e}")
        sys.exit(1)
        # éªŒè¯TFLiteæ¨¡å‹
    # ä½¿ç”¨
    #analyze_model_details(decompressed_bytes)
    # è·å–æ‰€æœ‰å¼ é‡è¯¦ç»†ä¿¡æ¯
    tensor_details = interpreter.get_tensor_details()
    print("TFLite æ¨¡å‹å®Œæ•´å±‚ä¿¡æ¯")
    print("=" * 80)
    print(f"{'ç´¢å¼•':<5} {'åç§°':<25} {'å½¢çŠ¶':<15} {'æ•°æ®ç±»å‹':<12} {'é‡åŒ–ä¿¡æ¯'}")
    print("=" * 80)

    for tensor in tensor_details:
        print(
            f"{tensor['index']:<5} {tensor['name']:<25} {str(tensor['shape']):<15} {str(tensor['dtype']):<12} {tensor.get('quantization', 'None')}")

    print("TFLite æ¨¡å‹å®Œæ•´å±‚ä¿¡æ¯")
    print("=" * 80)
    print(f"{'ç´¢å¼•':<5} {'åç§°':<25} {'å½¢çŠ¶':<15} {'æ•°æ®ç±»å‹':<12} {'é‡åŒ–ä¿¡æ¯'}")
    print("=" * 80)

    for tensor in tensor_details:
        # æ­£ç¡®å¤„ç†é‡åŒ–ä¿¡æ¯ï¼ˆå¯èƒ½æ˜¯å…ƒç»„æˆ–Noneï¼‰
        quantization = tensor.get('quantization', ())

        if quantization and isinstance(quantization, (list, tuple)) and len(quantization) >= 2:
            scale, zero_point = quantization[0], quantization[1]
            quant_info = f"scale:{scale}, zero_point:{zero_point}"
        else:
            quant_info = "quantization free"

        print(
            f"{tensor['index']:<5} {str(tensor['name'])[:24]:<25} {str(tensor['shape']):<15} {str(tensor['dtype']):<12} {quant_info}")

    print("=" * 80)
    print(f" æ€»å±‚æ•°: {len(tensor_details)}")
    print(f" æ¨¡å‹å¤§å°: {len(decompressed_bytes):,} å­—èŠ‚")
    path_policy_tflite = os.path.join(MODEL_DIR, "esp32_optimized_model.tflite")
    with open(path_policy_tflite, 'wb') as f:
        f.write(decompressed_bytes)

    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {path_policy_tflite}")
    print(f"æ¨¡å‹å¤§å°: {len(decompressed_bytes)} å­—èŠ‚")

    # ä½¿ç”¨Post-Training Quantizationè¿›è¡Œé‡åŒ–
    #model = tf.keras.models.load_model(path_policy_h5)

    #quantize_model = tf.quantization.quantize(model)

    # å°†é‡åŒ–åçš„æ¨¡å‹ä¿å­˜ä¸º TFLite æ ¼å¼
    #converter = tf.lite.TFLiteConverter.from_keras_model(quantize_model)
    #converter.optimizations = [tf.lite.Optimize.DEFAULT]

    # è®¾ç½®æ”¯æŒé‡åŒ–çš„æ“ä½œ
    #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    #converter.representative_dataset = representative_data
    #tflite_model = converter.convert()
    #with open(path_policy_tflite, 'wb') as f:
    #    f.write(tflite_model)
    # è¿è¡Œè°ƒè¯•
    debug_model_creation(representative_data)
    # åœ¨ä¿å­˜åç«‹å³éªŒè¯
    if verify_tflite_model(path_policy_tflite):
        print("PCç«¯éªŒè¯é€šè¿‡ï¼Œå¯ä»¥ä¸Šä¼ åˆ°ESP32")
    else:
        print("PCç«¯éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹")
    # ä¹Ÿå¯å–®ç¨å‘¼å«
    #_ = exporter.apply_quantization(representative_data)
    #_ = exporter.compute_fisher_matrix(representative_data)
