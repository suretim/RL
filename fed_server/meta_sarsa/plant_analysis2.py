import time
from datetime import datetime
import matplotlib.pyplot as plt
import pandas as pd


class QModel:
    def __init__(self, state_size=None, action_size=3, learning_rate=0.1, gamma=0.9,
                 initial_epsilon=0.3, config_file="best_params.json"):
        # ... ä¹‹å‰çš„åˆå§‹åŒ–ä»£ç¢¼ ...

        # è¨“ç·´éç¨‹ç›£æ§
        self.training_history = {
            'episodes': [],
            'rewards': [],
            'avg_rewards': [],
            'epsilons': [],
            'q_table_sizes': [],
            'exploration_rates': [],
            'losses': [],
            'timestamps': [],
            'action_distributions': []
        }

        self.current_episode = 0
        self.episode_rewards = []
        self.episode_start_time = time.time()

        print(f"âœ… QModel åˆå§‹åŒ–å®Œæˆ: å‹•ä½œæ•¸={action_size}, åƒæ•¸={params}")

    def start_episode(self):
        """é–‹å§‹æ–°çš„è¨“ç·´å›åˆ"""
        self.current_episode += 1
        self.episode_rewards = []
        self.episode_start_time = time.time()
        self.episode_exploration_count = 0
        self.episode_exploitation_count = 0

    def end_episode(self):
        """çµæŸç•¶å‰è¨“ç·´å›åˆä¸¦è¨˜éŒ„çµ±è¨ˆ"""
        if not self.episode_rewards:
            return

        total_reward = sum(self.episode_rewards)
        avg_reward = total_reward / len(self.episode_rewards)
        episode_duration = time.time() - self.episode_start_time

        # è¨˜éŒ„è¨“ç·´æ­·å²
        self.training_history['episodes'].append(self.current_episode)
        self.training_history['rewards'].append(total_reward)
        self.training_history['avg_rewards'].append(avg_reward)
        self.training_history['epsilons'].append(self.epsilon)
        self.training_history['q_table_sizes'].append(len(self.q_table))
        self.training_history['timestamps'].append(datetime.now())
        self.training_history['losses'].append(0)  # å¯ä»¥æ ¹æ“šéœ€è¦è¨ˆç®—æå¤±

        # è¨ˆç®—æ¢ç´¢ç‡
        total_choices = self.episode_exploration_count + self.episode_exploitation_count
        if total_choices > 0:
            exploration_rate = (self.episode_exploration_count / total_choices) * 100
        else:
            exploration_rate = 0

        self.training_history['exploration_rates'].append(exploration_rate)

        # è¨˜éŒ„å‹•ä½œåˆ†ä½ˆ
        action_dist = self.exploration_stats['action_distribution'].copy()
        self.training_history['action_distributions'].append(action_dist)

    def update(self, state, action, reward, next_state, next_action=None):
        """
        æ›´æ–° Q-table ä¸¦è¨˜éŒ„è¨“ç·´æ•¸æ“š
        """
        # è¨˜éŒ„çå‹µ
        self.episode_rewards.append(reward)

        # è¨˜éŒ„æ¢ç´¢/åˆ©ç”¨
        discrete_state = self._discretize_state(state)
        q_values = self.q_table[discrete_state]
        if np.random.random() < self.epsilon:
            self.episode_exploration_count += 1
        else:
            self.episode_exploitation_count += 1

        # åŸæœ‰çš„æ›´æ–°é‚è¼¯
        discrete_next_state = self._discretize_state(next_state)
        current_q = self.q_table[discrete_state][action]

        if next_action is not None:
            next_q = self.q_table[discrete_next_state][next_action]
        else:
            next_q = np.max(self.q_table[discrete_next_state])

        new_q = current_q + self.learning_rate * (
                reward + self.gamma * next_q - current_q
        )

        self.q_table[discrete_state][action] = new_q

        # è¡°æ¸›æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def check_training_process(self, detailed=False, plot=False):
        """
        æª¢æŸ¥è¨“ç·´éç¨‹ç‹€æ…‹

        Args:
            detailed: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
            plot: æ˜¯å¦ç”Ÿæˆè¨“ç·´æ›²ç·šåœ–

        Returns:
            è¨“ç·´éç¨‹çµ±è¨ˆå­—å…¸
        """
        if not self.training_history['episodes']:
            return {"status": "å°šæœªé–‹å§‹è¨“ç·´"}

        current_episode = self.current_episode
        latest_reward = self.training_history['rewards'][-1] if self.training_history['rewards'] else 0
        avg_reward = self.training_history['avg_rewards'][-1] if self.training_history['avg_rewards'] else 0

        stats = {
            'current_episode': current_episode,
            'total_episodes': len(self.training_history['episodes']),
            'latest_reward': round(latest_reward, 2),
            'latest_avg_reward': round(avg_reward, 2),
            'current_epsilon': round(self.epsilon, 4),
            'q_table_size': len(self.q_table),
            'training_duration': self._get_training_duration(),
            'overall_avg_reward': round(np.mean(self.training_history['rewards']), 2) if self.training_history[
                'rewards'] else 0,
            'best_reward': round(max(self.training_history['rewards']), 2) if self.training_history['rewards'] else 0,
            'convergence_status': self._check_convergence()
        }

        if detailed:
            # æ·»åŠ è©³ç´°çµ±è¨ˆ
            stats.update({
                'reward_trend': self._get_reward_trend(),
                'exploration_trend': self._get_exploration_trend(),
                'recent_performance': self._get_recent_performance(10),
                'action_distribution': self._get_current_action_distribution()
            })

        if plot:
            self._plot_training_progress()

        return stats

    def _get_training_duration(self):
        """è¨ˆç®—ç¸½è¨“ç·´æ™‚é–“"""
        if not self.training_history['timestamps']:
            return "0s"

        start_time = self.training_history['timestamps'][0]
        end_time = self.training_history['timestamps'][-1]
        duration = (end_time - start_time).total_seconds()

        if duration < 60:
            return f"{int(duration)}s"
        elif duration < 3600:
            return f"{int(duration // 60)}m {int(duration % 60)}s"
        else:
            return f"{int(duration // 3600)}h {int((duration % 3600) // 60)}m"

    def _check_convergence(self):
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¶æ–‚"""
        if len(self.training_history['rewards']) < 20:
            return "éœ€è¦æ›´å¤šè¨“ç·´æ•¸æ“š"

        recent_rewards = self.training_history['rewards'][-10:]
        reward_std = np.std(recent_rewards)

        if reward_std < 5:  # çå‹µæ³¢å‹•å¾ˆå°
            return "å¯èƒ½å·²æ”¶æ–‚"
        elif self.epsilon <= self.epsilon_min + 0.01:
            return "æ¢ç´¢ç‡å·²ç©©å®š"
        else:
            return "ä»åœ¨å­¸ç¿’ä¸­"

    def _get_reward_trend(self):
        """ç²å–çå‹µè¶¨å‹¢"""
        if len(self.training_history['rewards']) < 5:
            return "æ•¸æ“šä¸è¶³"

        recent_rewards = self.training_history['rewards'][-5:]
        if all(recent_rewards[i] <= recent_rewards[i + 1] for i in range(len(recent_rewards) - 1)):
            return "ä¸Šå‡"
        elif all(recent_rewards[i] >= recent_rewards[i + 1] for i in range(len(recent_rewards) - 1)):
            return "ä¸‹é™"
        else:
            return "æ³¢å‹•"

    def _get_exploration_trend(self):
        """ç²å–æ¢ç´¢è¶¨å‹¢"""
        if len(self.training_history['exploration_rates']) < 5:
            return "æ•¸æ“šä¸è¶³"

        recent_rates = self.training_history['exploration_rates'][-5:]
        return round(np.mean(recent_rates), 2)

    def _get_recent_performance(self, n=10):
        """ç²å–æœ€è¿‘nå€‹å›åˆçš„æ€§èƒ½"""
        if len(self.training_history['rewards']) < n:
            n = len(self.training_history['rewards'])

        recent_rewards = self.training_history['rewards'][-n:]
        return {
            'avg_reward': round(np.mean(recent_rewards), 2),
            'min_reward': round(min(recent_rewards), 2),
            'max_reward': round(max(recent_rewards), 2),
            'std_reward': round(np.std(recent_rewards), 2)
        }

    def _get_current_action_distribution(self):
        """ç²å–ç•¶å‰å‹•ä½œåˆ†ä½ˆ"""
        total = sum(self.exploration_stats['action_distribution'])
        if total == 0:
            return {f'action_{i}': 0 for i in range(self.action_size)}

        return {
            f'action_{i}': round((count / total) * 100, 2)
            for i, count in enumerate(self.exploration_stats['action_distribution'])
        }

    def _plot_training_progress(self):
        """ç¹ªè£½è¨“ç·´é€²åº¦åœ–"""
        if len(self.training_history['episodes']) < 2:
            print("éœ€è¦æ›´å¤šæ•¸æ“šä¾†ç¹ªåœ–")
            return

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # çå‹µæ›²ç·š
        ax1.plot(self.training_history['episodes'], self.training_history['rewards'])
        ax1.set_title('Total Reward per Episode')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Reward')
        ax1.grid(True)

        # å¹³å‡çå‹µæ›²ç·š
        ax2.plot(self.training_history['episodes'], self.training_history['avg_rewards'])
        ax2.set_title('Average Reward per Episode')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Average Reward')
        ax2.grid(True)

        # æ¢ç´¢ç‡æ›²ç·š
        ax3.plot(self.training_history['episodes'], self.training_history['exploration_rates'])
        ax3.set_title('Exploration Rate')
        ax3.set_xlabel('Episode')
        ax3.set_ylabel('Exploration Rate (%)')
        ax3.grid(True)

        # Q-table å¤§å°æ›²ç·š
        ax4.plot(self.training_history['episodes'], self.training_history['q_table_sizes'])
        ax4.set_title('Q-table Size')
        ax4.set_xlabel('Episode')
        ax4.set_ylabel('Number of States')
        ax4.grid(True)

        plt.tight_layout()
        plt.savefig('training_progress.png')
        plt.close()

        print("è¨“ç·´é€²åº¦åœ–å·²ä¿å­˜ç‚º 'training_progress.png'")

    def get_training_summary(self):
        """ç²å–è¨“ç·´æ‘˜è¦å ±å‘Š"""
        stats = self.check_training_process(detailed=True)

        summary = f"""
        ğŸ¯ è¨“ç·´éç¨‹æ‘˜è¦
        ========================
        ç¸½å›åˆæ•¸: {stats['total_episodes']}
        ç•¶å‰å›åˆ: {stats['current_episode']}
        è¨“ç·´æ™‚é•·: {stats['training_duration']}

        ğŸ“Š çå‹µçµ±è¨ˆ:
        æœ€æ–°çå‹µ: {stats['latest_reward']}
        å¹³å‡çå‹µ: {stats['overall_avg_reward']}
        æœ€ä½³çå‹µ: {stats['best_reward']}
        çå‹µè¶¨å‹¢: {stats['reward_trend']}

        âš™ï¸ æ¨¡å‹ç‹€æ…‹:
        Q-table å¤§å°: {stats['q_table_size']} å€‹ç‹€æ…‹
        ç•¶å‰æ¢ç´¢ç‡: {stats['current_epsilon']}
        æ¢ç´¢è¶¨å‹¢: {stats['exploration_trend']}%
        æ”¶æ–‚ç‹€æ…‹: {stats['convergence_status']}

        ğŸ¯ æœ€è¿‘è¡¨ç¾:
        å¹³å‡çå‹µ: {stats['recent_performance']['avg_reward']}
        æ³¢å‹•ç¯„åœ: Â±{stats['recent_performance']['std_reward']}
        """

        return summary


# ä½¿ç”¨ç¤ºä¾‹
class TrafficAnalysis:
    def __init__(self):
        self.q_model = QModel(action_size=3)
        self.ACTIONS = {0: "NORMAL", 1: "LIMIT", 2: "BLOCK"}

    def train_episode(self, traffic_data_list):
        """è¨“ç·´ä¸€å€‹å›åˆ"""
        self.q_model.start_episode()

        for i, traffic_data in enumerate(traffic_data_list):
            state = self._extract_features(traffic_data)
            action = self.q_model.choose_action(state)
            reward = self._calculate_reward(traffic_data, action)
            next_state = self._extract_features(self._get_next_data())

            self.q_model.update(state, action, reward, next_state)

            # æ¯10æ­¥æª¢æŸ¥ä¸€æ¬¡è¨“ç·´éç¨‹
            if i % 10 == 0:
                training_info = self.q_model.check_training_process()
                print(f"æ­¥é©Ÿ {i}: çå‹µ={reward}, æ¢ç´¢ç‡={training_info['current_epsilon']}")

        self.q_model.end_episode()

        # æ¯å€‹å›åˆçµæŸå¾Œæª¢æŸ¥
        if self.q_model.current_episode % 5 == 0:
            summary = self.q_model.get_training_summary()
            print(summary)

            # æ¯10å€‹å›åˆç¹ªåœ–ä¸€æ¬¡
            if self.q_model.current_episode % 10 == 0:
                self.q_model.check_training_process(plot=True)


# å¿«é€Ÿæ¸¬è©¦
if __name__ == "__main__":
    model = QModel(action_size=3)

    # æ¨¡æ“¬è¨“ç·´éç¨‹
    for episode in range(1, 21):
        model.start_episode()

        # æ¨¡æ“¬æ¯å€‹å›åˆçš„10å€‹æ­¥é©Ÿ
        for step in range(10):
            state = [np.random.random() for _ in range(3)]
            action = model.choose_action(state)
            reward = np.random.randint(-5, 15)
            next_state = [np.random.random() for _ in range(3)]

            model.update(state, action, reward, next_state)

        model.end_episode()

        # æ¯5å€‹å›åˆæª¢æŸ¥ä¸€æ¬¡
        if episode % 5 == 0:
            stats = model.check_training_process(detailed=True)
            print(f"å›åˆ {episode}: çå‹µ={stats['latest_reward']}, ç‹€æ…‹æ•¸={stats['q_table_size']}")

    # æœ€çµ‚å ±å‘Š
    print("\n" + "=" * 50)
    print("æœ€çµ‚è¨“ç·´å ±å‘Š:")
    print(model.get_training_summary())
    model.check_training_process(plot=True)